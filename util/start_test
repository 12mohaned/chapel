#!/usr/bin/env python 
#
# CHAPEL TESTING

import argparse
import sys
import time
import os
import subprocess
import platform
import re
import getpass
import shutil
import tempfile
import fnmatch
import glob
import contextlib
import test.convert_start_test_log_to_junit_xml
from chplenv import *


args = None

# PROGRAM ENTRY POINT
def main():
	# check environment
	check_environment()

	# set up the parser and parse command-line arguments
	parser = parser_setup()
	global args
	(args, opts) = parser.parse_known_args()
	parse_passthrough_opts()
	
	invocationDir = os.getcwd()

	run_tests(args.tests)

	# in case we've changed directories
	os.chdir(invocationDir)

	cleanup()

	finish()

	print
	print


def run_tests(tests):
	files = []
	dirs = []
	for i in tests:
		if os.path.isdir(i) : # a directory (also get absolute path)
			dirs.append(os.path.abspath(i))
		elif os.path.isfile(i): # a file
			files.append(i)
		else:
			print "[Error: %s is not a valid file or directory]" % i

	# set up
	set_up_environment()
	set_up_general()
	set_up_performance_testing_A()
	set_up_executables()
	set_up_performance_testing_B()
	# autogenerate tests from spec if no tests were given
	if len(files) == 0 and len(dirs) == 0: # no tests specified
		auto_generate_tests()
		if os.getcwd() == home:
			dirs = [testdir]
		else:
			dirs = [os.getcwd()]
	
	# print out flags
	print '[tests: "%s"]' % " ".join(files)
	if args.recurse:
		print '[directories: "%s"]' % " ".join(dirs) 
	else:
		print '[directories: (nonrecursive): "%s"]' % " ".join(dirs)

	# check for duplicate .graph and .dat files
	check_for_duplicates()

	# print out Chapel environment
	print_chapel_environment()

	# when the user specifies specific files, run them, even if they are 
	# futures or notests
	os.environ["CHPL_TEST_FUTURES"] = "1"
	os.environ["CHPL_TEST_NOTESTS"] = "1"
	os.environ["CHPL_TEST_SINGLES"] = "1"

	for test in files:
		test_file(test)

	os.environ["CHPL_TEST_FUTURES"] = str(args.futuresMode)
	os.environ["CHPL_TEST_NOTESTS"] = "0"
	os.environ["CHPL_TEST_SINGLES"] = "0"

	# set up multiple passes/runs through the directories
	if args.performance:
		testruns = ["performance", "graph"]
	else:
		if args.gengraphs:
			testruns = ["graph"]
		else:
			testruns = ["run"]

	for tests in dirs:
		for t in testruns:
			test_directory(tests, t)

	# test and graph compiler performance
	if args.compperformance:
		compiler_performance()

	# generate GRAPHFILES graphs
	if args.gengraphs:
		generate_graphfiles_graphs()


# ESCAPE ROUTINES AND CLEAN-UP

def finish():
	# summarize
	if not args.cleanonly:
		summarize()
	else:
		print "[Summary: CLEAN ONLY]"
	print

	# exit, returning 0 if no failures and 2 if there were some
	if failures == 0:
		sys.exit(0)
	else:
		sys.exit(2) 


# MAIN ROUTINES AND TESTING

def test_file(test):
	pathtotest = os.path.relpath(test)
	testname = os.path.basename(test)
	
	with cd(os.path.dirname("./" + test)): # cd into dir, and cd out later
		# clean executables, etc
		print
		print "[Cleaning file %s]" % test
		# clean and run test
		clean(testname)

		error = 0
		if not args.cleanonly:
			if args.performance or not args.gengraphs:
				error = run(test)
		
			# check for errors - 173 is an internal sub_test error that would
			# have already reported.
			if error != 0 and error != 173:
				print "[Error running sub_test for %s]" % pathtotest

			if args.progress:
				sys.stderr.write("[done]\n")

			del os.environ["CHPL_ONETEST"]
			
			if args.gengraphs:
				generate_graphs(test)


def test_directory(test, testtype):
	print "[Working from directory %s]" % test

	# recurse through directory
	for root, dirs, files in os.walk(test):
		if not os.access(root, os.X_OK):
			print "[Warning: Cannot cd into %s skipping directory]" % dir
			continue
		else:
			dir = os.path.abspath(root)

		print
		print "[Working on directory %s]" % root

		# stop recursing if flag is set 
		if not args.recurse:
			del dirs[:] 

		# ignore skipifs for --clean-only run
		if not args.cleanonly:
			with cd(dir): # cd into dir, and cd out later
				# SKIP IF IMPLEMENTATIONS
				testenv = os.path.join(utildir, "test/testEnv")
				
				# Skip the directory if there is a SKIPIF file that 
				# evaluates true
				skiptest = False
				skipifpath = os.path.join(root, "SKIPIF")
				if os.path.isfile("SKIPIF") :
					# if executable
					if os.access("./SKIPIF", os.X_OK):
						skiptest = getOutput(["./SKIPIF"])
					else: # if not, run a script to check SKIPIF condition
						skiptest = getOutput([testenv, "SKIPIF"]) 
					# check output and skip if true
					if skiptest == "1" or skiptest == "True" :
						print ("[Skipping directory based on SKIPIF "		
								"environment settings]")
						continue
				
				# Skip this directory if there is a <dir>.skipif file 
				# returning true
				pruneif = False
				skipfilename = os.path.join(root, 
						"%s.skipif" % os.path.basename(root))
				if os.path.isfile(skipfilename) :
					# if executable
					if os.access(skipfilename, os.X_OK):
						pruneif = getOutput([skipfilename])
					else: # if not, run a script to check SKIPIF condition
						pruneif = getOutput([testenv, skipfilename])
					# check output and skip if true
					if pruneif == "1" or pruneif == "True" :
						print ("[Skipping directory and children based on "
						".skipif environment settings in %s]") % skipfilename
						del dirs[:]
						continue

		# run tests in directory
		# don't run if only doing performance graphs
		if testtype != "graph":
			# check for .chpl or .test.c files, and for NOTEST
			aretests = False
			for f in files:
				if testtype != "performance":
					if f.endswith(".chpl") or f.endswith(".test.c"):
						aretests = True
						break
				else:
					if f.endswith("." + perfkeys):
						aretests= True
						break;
			
			# check a lot of stuff before continuing
			if (not os.path.isfile(os.path.join(dir, "NOTEST")) and	(aretests 
				or os.access(os.path.join(dir, "sub_test"), os.X_OK))):
				# cd to dir for clean and run, saving current location 
				with cd(dir):
					# clean dir
					clean()

					if not args.cleanonly:
						# run all tests in dir
						error = run()
						# check for errors - 173 is an internal sub_test error  
						# that would have already reported.
						if error != 0 and error != 173:
							print "[Error running sub_test in %s (%i)]" % (root,
									error)
			# let user know no tests were found
			else:
				print "[No tests in directory %s]" % root
		# generate graphs
		else:
			with cd(dir):
				# generate graphs for all testsin dir
				generate_graphs()


def summarize():
	datestr = time.strftime("%y%m%d.%H%M%S");
	print "[Done with tests - %s]" % datestr
	print "[Log file: %s ]" % os.path.abspath(logfile)
	print
	sys.stdout.flush()
	# setup regular expressions for searching
	futuremarker = r"^Future"
	suppressmarker = r"^Suppress"
	errormarker = r"^\[Error"
	if not args.performance and args.gengraphs:
		successmarker = r"\[Success generating"
	else:
		if args.componly:
			successmarker = r"\[Success compiling"
		else:
			successmarker = r"\[Success matching"
	warningmarker = r"^\[Warning"
	passingSuppressionsMarker = "%s.*%s" % (suppressmarker, successmarker)
	passingFuturesMarker = "%s.*%s" % (futuremarker, successmarker)
	successmarker = "^" + successmarker
	skipstdinredirectmarker = r"^\[Skipping test with .stdin input"

	# setup counts and blank strings to hold summaries
	global failures # for exit codes later
	failures = 0
	successes = 0
	futures = 0
	warnings = 0
	passingsuppressions = 0
	passingfutures = 0
	skipstdinredirs = 0
	failure_summary = ""
	suppression_summary = ""
	future_summary = ""
	warning_summary = ""
	summary = "[Test Summary - %s]\n" % datestr

	# scan line-by-line, logging failures, warnings, etc.. and updating counts
	log = open(logfile, "r")
	for line in log:
		if re.search(errormarker, line, flags=re.M):
			failure_summary += line
			failures += 1
		elif re.search(suppressmarker, line, flags=re.M):
			suppression_summary += line
		elif re.search(futuremarker, line, flags=re.M):
			future_summary += line
			futures += 1
		elif re.search(warningmarker, line, flags=re.M):
			warning_summary += line
			warnings += 1
		elif re.search(successmarker, line, flags=re.M):
			successes += 1
		if re.search(passingSuppressionsMarker, line, flags=re.M):
			passingsuppressions += 1
		elif re.search(passingFuturesMarker, line, flags=re.M):
			passingfutures += 1
		elif re.search(skipstdinredirectmarker, line, flags=re.M):
			skipstdinredirs += 1

	log.close()

	# compile summary
	summary += failure_summary
	summary += suppression_summary
	summary += future_summary
	summary += warning_summary
	
	if skipstdinredirs > 0:
		print "[Skipped %s tests with .stdin input]" % skipstdinredirs

	summary += ("[Summary: #Successes = %i | #Failures = %i | #Futures = %i | " 			 "#Warnings = %i ]\n") % (successes, failures, futures, warnings)
	summary += ("[Summary: #Passing Suppressions = %i | #Passing Futures "
			"= %i ]") % (passingsuppressions, passingfutures)
	# the actual running is done later, but in order to include it in
	# the summary file we print it out here.  It all happens within 
	# a split second anyway
	if args.junit_xml:
		summary += "[Generating jUnit XML report]\n"
		summary += jUnit()
	summary += "\n[END]"

	# log summary, and write it to its own .summary file
	print summary

	logsummary = open(logfile + ".summary", "w")	
	logsummary.write(summary)
	logsummary.close()


def clean(test = False):
	datestr = time.strftime("%a %b %d %H:%M:%S %Z %Y")
	
	# clean executables, tmps, etc.
	sub_clean = os.path.join(utildir, "test/sub_clean")
	if test: # single test
		print "[Starting %s %s %s]" % (sub_clean, test, datestr)
		p = subprocess.Popen([sub_clean, test], stdout=subprocess.PIPE)
	else:
		p = subprocess.Popen([sub_clean], stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	

def run(test = False):
	datestr = time.strftime("%a %b %d %H:%M:%S %Z %Y")
	os.environ["CHPL_TEST_UTIL_DIR"] = utildir
	
	# run test
	print
	if test: # single test
		print "[Working on file %s]" % os.path.relpath(test)
		os.environ["CHPL_ONETEST"] = os.path.basename(test)

	if os.access("sub_test", os.X_OK):
		sub_test = os.path.abspath("sub_test")
	else:
		sub_test = os.path.join(utildir, "test/sub_test")

	if args.progress and test:
		sys.stderr.write("Testing %s ... \n" % test)

	print "[Starting %s %s]" % (sub_test, datestr)
	p = subprocess.Popen([sub_test, compiler], stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	return p.returncode


def generate_graphs(test = False):
	if test: 
		basedir = os.path.dirname(test)
		graphfiles = [(test.replace(".chpl","").replace(".test.c","") 
				+ ".graph")]
		# exit if it isn't actually a file
		if not os.path.isfile(graphfiles[0]):
			return
	else:
		basedir = os.getcwd()
		graphfiles = glob.glob("*.graph");
		# exit if no files matched
		if len(graphfiles) == 0 or os.environ.get("CHPL_TEST_PERF_DIR"):
			return

	print "[Executing genGraphs for graphfiles in %s in %s" % (basedir,
			perfhtmldir)

	p = subprocess.Popen([createGraphs, args.gengraphopts,
		"-p", perfdir, "-o", perfhtmldir, "-n", perftestname +
		startdate_t, args.graphsdisprange, "-r", 
		args.graphsgendefault, " ".join(graphfiles)], 
		stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	status = p.returncode
	
	if status == 0:
		print "[Success generating graphs for graphfiles in %s in %s" % (
				basedir, perfhtmldir)
	else:
		print "[Error generating graphs for graphfiles in %s in %s" % (
				basedir, perfhtmldir)


def compiler_performance():
	end_time = int(time.time())
	elapsed = end_time - start_time

	compGraphList = os.path.join(testdir, "COMPGRAPHFILES")

	# combine smaller .dat files
	print "[Combining dat files now]"
	p = subprocess.Popen([combineCompPerf, "--tempDatDir", tempDatFilesDir,
		"--elapsedTestTime", str(elapsed), "--outdir", compperfdir],
		stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	if p.returncode == 0:
		print "[Success combining compiler performance dat files]"
	else:
		print "[Error combining compiler performance dat files]"

	# create the graphs
	atitle = "Chapel Compiler Performance Graphs"
	print "[Creating compiler perforamce graphs now]"
	p = subprocess.Popen([createGraphs, args.gengraphopts, "-p", compperfdir, 
		"-o", compperfhtmldir, "-a", atitle, "-n", (compperftestname + 
		startdate_t), "-g", compGraphList, "-t", testdir, 
		"-m all:v,examples:v", "-x"], stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	if p.returncode == 0:
		print ("[Success generating compiler perfrommance graphs from %s in %s]"
			% (compGraphList, compperfhtmldir))
	else:
		print ("[Error generating compiler perfrommance graphs from %s in %s]"
			% (compGraphList, compperfhtmldir))
	
	# delete temp files
	shutil.rmtree(tempDatFilesDir)


def generate_graphfiles_graphs():
	execGraphList = os.path.join(testdir, "GRAPHFILES")

	print "[Executing genGraphs for %s in %s" % (execGraphList,	perfhtmldir)

	p = subprocess.Popen([createGraphs, args.gengraphopts,
		"-p", perfdir, "-o", perfhtmldir, "-t", testdir, "-n", (perftestname +
		startdate_t), args.graphsdisprange, "-r", args.graphsgendefault, "-g",
		execGraphList], stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	status = p.returncode
	
	if status == 0:
		print "[Success generating graphs from %s in %s" % (
				execGraphList, perfhtmldir)
	else:
		print "[Error generating graphs from %s in %s" % (
				execGraphList, perfhtmldir)


# SET UP ROUTINES

def check_environment():
 	if os.environ.get("CHPL_DEVELOPER") != None : # unset CHPL_DEVELOPER
		del os.environ["CHPL_DEVELOPER"]
	
	# Check for $CHPL_HOME
	global home
	if os.environ.get("CHPL_HOME") != None :
		home = os.environ["CHPL_HOME"]
		if not os.path.isdir(home):
			print "Error: CHPL_HOME must be a legal directory."
			sys.exit(1)
	
	# Allow for utility directory override. 
	# Useful when running more recent testing system on an older version of
	# Chapel.
	global utildir
	if os.environ.get("CHPL_TEST_UTIL_DIR") != None :
		utildir = os.environ["CHPL_TEST_UTIL_DIR"]
	else:
		utildir = os.path.normpath(os.path.join(home, "util"))
		os.environ["CHPL_TEST_UTIL_DIR"] = utildir
		if not os.path.isdir(utildir) :
			print "Error: Cannot find %s." % utildir
			sys.exit(1)

	# create temporary directory
	global chpltesttmpdir
	if os.environ.get("CHPL_TEST_TMP_DIR") != None:
		chpltesttmpdir = os.environ["CHPL_TEST_TMP_DIR"]
	else:
		chpltesttmpdir = tempfile.mkdtemp(prefix="chplTestTmpDir.")
		os.environ["CHPL_TEST_TMP_DIR"] = chpltesttmpdir

	# find test dir and check for access
	global testdir
	testdir = os.path.join(home, "test")
	if (not os.access(testdir, os.F_OK) or not os.access(testdir, os.W_OK)
		or not os.access (testdir, os.X_OK)):
		testdir = os.path.join(home, "examples")
		if (not os.access(testdir, os.F_OK) or not os.access(testdir, os.W_OK)
			or not os.access (testdir, os.X_OK)):
			testdir = os.getcwd()
	if (not os.access(testdir, os.F_OK) or not os.access(testdir, os.W_OK)
		or not os.access (testdir, os.X_OK)):
		print "Cannot write to test directory %s" % testdir
		sys.exit(-1)

	# find Logs directory and check for access
	global logsdir
	logsdir = os.path.join(testdir, "Logs")
	if not os.path.isdir(logsdir):
		os.mkdir(logsdir)
	if not os.access(logsdir, os.W_OK):
		print "Cannot write to Logs directory %s" % logsdir
		sys.exit(-1)

	# save host and target platforms, and configure environment appropriately
	global host_platform, tgt_platform
	host_platform = chpl_platform.get("host")
	tgt_platform = chpl_platform.get("target") 
	if tgt_platform != "sunos":
		os.environ["LC_ALL"] = "C"
		os.environ["LANG"] = "en_US"

	global logfile
	logfile = os.path.join(logsdir, "%s.%s.log" % (getpass.getuser(),
		tgt_platform))


def set_up_environment():
	if args.memleakslog:
		os.environ["CHPL_MEM_LEAK_TESTING"] = "true"
		args.execopts += " --memLeaksLog=" + os.path.abspath(args.memleakslog)

	# pre-
	if args.preexec:
		os.environ["CHPL_SYSTEM_PREEXEC"] = args.syspreexec
	
	if args.prediff:
		os.environ["CHPL_SYSTEM_PREDIFF"] = args.sysprediff

	# compopts
	if args.compopts:
		args.compopts = "--cc-warnings " + args.compopts
	else:
		args.compopts = "--cc-warnings"

	# performance (linking flags to each other)
	if args.performance or args.performancedescription:
		args.performance = True
		args.gengraphs = True
		args.compopts += " --fast"
		if tgt_platform != "darwin":
			args.compopts += " --static"

	if args.performanceconfigs:
		args.gengraphopts += " --configs" + args.gengraphopts

	if args.compperformancedescription:
		args.compperformance = True

	# compiler only
	if args.componly:
		os.environ["CHPL_COMPONLY"] = "true"

	# stdin redirection
	if args.nostdinredirect or args.stdinredirect:
		os.environ["CHPL_NO_STDIN_REDIRECT"] = "true"
		if args.stdinredirect:
			os.environ["CHPL_TEST_FORCE_STDIN_REDIRECT"] = "true"

	# launcher timeout
	if args.launchertimeout:
		os.environ["CHPL_LAUNCHER_TIMEOUT"] = str(args.launchertimeout)
	
	# num trials
	os.environ["CHPL_TEST_NUM_TRIALS"] = str(args.numtrials)

	# test root dir
	if args.testrootdir:
		os.environ["CHPL_TEST_ROOT_DIR"] = str(args.testrootdir)

	# jUnit
	if args.junit_xml_file:
		args.junit_xml = True
	

def set_up_general():
	# logfile
	global logfile 
	if args.logfile: 
		logfile = os.path.abspath(args.logfile)
	logfiledir = os.path.dirname(logfile)
	if not os.access(logfiledir, os.X_OK):
		print "[Permission denied for logfile directory: %s]" % logfiledir
		sys.exit(1)
	if os.path.isfile(logfile):
		print
		print "[Removing log file with duplicate name %s" % logfile
		os.remove(logfile)
	
	## START LOGGING TO FILE ##
	sys.stdout = Logger()

	# start time
	global start_time
	if args.compperformance:
		start_time = int(time.time())

	print ("[Starting Chapel regression tests - %s]" % 
		time.strftime("%y%m%d.%H%M%S") )

	# check to see if we are in subdir of CHPL_HOME 
	if args.chplhomewarn and os.getcwd().find(home) < 0: 
		print ("[Warning: start_test not invoked from a subdirectory of "
				"$CHPL_HOME]")

	# log some messages
	print '[starting directory: "%s"]' % os.getcwd()
	print '[Logs directory: "%s"]' % logsdir
	print '[logfile: "%s"]' % logfile
	print "[CHPL_HOME: %s]" % home
	print "[host platform: %s]" % host_platform
	print "[target platform: %s]" % tgt_platform

	# valgrind
	if args.valgrind:
		print "[valgrind: ON]"
		p = subprocess.Popen("which valgrind", stdout=subprocess.PIPE)
		binary = p.stdout.readline()
		p.wait()
		if p.returncode != 0:
			print "[Error: Could not find valgrind.]"
			finish()
		p = subprocess.Popen("valgrind --version", stdout=subprocess.PIPE)
		version = p.stdout.readline()
		p.wait()
		print "[valgrind binary: %s]" % binary
		print "[valgrind version: %s]" % version
		os.environ["CHPL_TEST_VGRND_COMP"] = "on"
		os.environ["CHPL_TEST_VGRND_EXE"] = "on"
	else:
		os.environ["CHPL_TEST_VGRND_COMP"] = "off"
		if args.valgrindexe:
			print "[valgrind: EXE only]"
			os.environ["CHPL_TEST_VGRND_EXE"] = "on"
		else:
			print "[valgrind: OFF]"
			os.environ["CHPL_TEST_VGRND_EXE"] = "off"
	
	# compiler
	global compiler
	if not args.compiler:
		compiler = os.path.expandvars("$CHPL_HOME/bin/%s/chpl" % host_platform)
	else:
		compiler = args.compiler


def set_up_performance_testing_A():
	# performance 
	if args.performance:  
		print "[performance tests: ON]"
		os.environ["CHPL_TEST_PERF"] = "on"
		if args.perflabel != "perf":
			print "[performance label: %s]" % args.perflabel
		os.environ["CHPL_TEST_PERF_LABEL"] = args.perflabel
	else:
		print "[performance tests: OFF]"

	# compiler performance
	if args.compperformance:
		print "[compiler performance tests: ON]"
		os.environ["CHPL_TEST_COMP_PERF"] = "on"
	else:
		print "[compiler performance tests: OFF]"

	# this is here and not in setupexecutables for legacy compatibility reasons
	print "[number of trials: %s]" % os.environ["CHPL_TEST_NUM_TRIALS"]

	# graphs
	if args.gengraphs:
		print "[performance graph generation: ON]"
		if args.graphsdisprange:
			print "[performance graph ranges: ON]"
			args.graphsdisprange = ""
		else:
			print "[performance graph ranges: OFF]"
			args.graphsdisprange = "--no-bounds"
		print "[performance graph data reduction: %s]" % args.graphsgendefault
	else:
		print "[performance graph generation: OFF]"


def set_up_performance_testing_B():
	# common to performance and graphs
	if args.performance or args.gengraphs:
		# set global variables for later access and use
		global perfdir, perfhtmldir, perftestname
		
		perftestname = platform.node().split(".")[0].lower()

		if not os.environ.get("CHPL_TEST_PERF_DIR"):
			perfdir = os.path.expandvars("$CHPL_HOME/test/perfdat/" 
					+ perftestname)
			os.environ["CHPL_TEST_PERF_DIR"] = perfdir
			print ("[Warning: CHPL_TEST_PERF_DIR must be set for generating "
					"performance graphs, using default %s]") % perfdir
		else:
			perfdir = os.environ["CHPL_TEST_PERF_DIR"]

		perfhtmldir = os.path.join(perfdir, "html/")

		if (args.performancedescription != "" and args.performancedescription !=
			"default"):
			os.environ["CHPL_TEST_PERF_DESCRIPTION"] = (
				args.performancedescription)

	global perfkeys
	perfkeys = "%skeys" % args.perflabel

	# compiler
	if args.compperformance:
		# set global variables
		global compperftestname, compperfdir, tempDatFilesDir, compperfhtmldir
		global combineCompPerf

		compperftestname = platform.node().split(".")[0].lower()

		if os.environ.get("CHPL_TEST_COMP_PERF_DIR"):
			compperfdir = os.environ["CHPL_TEST_COMP_PERF_DIR"]
		else:
			compperfdir = os.path.join(home, ("test/compperfdat/%s/" %
					compperftestname))
			print ("[Warning: CHPL_COMP_TEST_PERF DIR must be set for "
					"generating compiler performance graphs, using default %s]"
					% compperfdir)

		compperftestname += args.compperformancedescription

		tempDatFilesDir = os.path.join(compperfdir, "tempCompPerfDatFiles/")
		os.environ["CHPL_TEST_COMP_PERF_TEMP_DAT_DIR"] = tempDatFilesDir
		#remove it if it wasn't cleaned up last time
		if os.path.isdir(tempDatFilesDir):
			shutil.rmtree(tempDatFilesDir)

		compperfhtmldir = os.path.join(compperfdir, "html/")

		combineCompPerf = os.path.join(utildir, "test/combineCompPerfData")
	
	# for any performance testing
	if args.performance or args.compperformance or args.gengraphs:
		global startdate_t, createGraphs

		if args.startdate:
			startdate_t = "-s" + startdate_t;
		else:
			startdate_t = ""

		createGraphs = os.path.join(utildir, "test/genGraphs")

		if not args.gengraphopts:
			args.gengraphopts = ""

	if args.performance or args.compperformance:
		# SHA for current run, in order to track dates to commits
		if args.performance:
			datdir = perfdir
		else:
			datdir = compperfdir

		if not os.path.isdir(datdir):
			os.mkdir(datdir)

		shaPerfKeysName = os.path.join(chpltesttmpdir, "sha.perfkeys")
		shaPerfKeys = open(shaPerfKeysName, "w")
		shaPerfKeys.write("sha ")
		shaPerfKeys.close()

		shaOutName = os.path.join(chpltesttmpdir, "sha.exec.out.tmp")
		shaOut = open(shaOutName, "w")
		p = subprocess.Popen(["git", "rev-parse", "HEAD"], 
				stdout=subprocess.PIPE)
		output = p.stdout.readline()
		p.wait()
		shaOut.write("sha " + output)
		shaOut.close()

		shaDatFileName = "perfSha"
		shaDatFilePath = os.path.join(datdir, "%s.dat" % shaDatFileName)
		os.environ["CHPL_TEST_SHA_DAT_FILE"] = shaDatFilePath

		print "[Saving current git sha to %s]" % shaDatFilePath
		cmd = os.path.join(utildir, "test/computePerfStats")
		p = subprocess.Popen([cmd, shaDatFileName, datdir, shaPerfKeysName,
			shaOutName], stdout=subprocess.PIPE)
		printout(p.stdout)
		p.wait()
		if p.returncode != 0:
			print "[Error: Failed to save current sha to %s]" % shaDatFilePath


def set_up_executables():
	# check for compiler
	global compiler
	if os.path.isfile(compiler) and os.access(compiler, os.X_OK):
		compiler = os.path.abspath(compiler)

		print '[compiler: "%s"]' % compiler
	else:
		print "[Error: Cannot find or execute compiler: %s]" % compiler
		finish()
	
	# log more options
	print '[compopts: "%s"]' % args.compopts
	os.environ['COMPOPTS'] = args.compopts

	print '[execopts: "%s"]' % args.execopts
	os.environ['EXECOPTS'] = args.execopts

	print '[launchcmd: "%s"]' % args.launchcmd
	os.environ['LAUNCHCMD'] = args.launchcmd

	comm = chpl_comm.get()
	launcher = chpl_launcher.get()
	locMod = chpl_locale_model.get()
	
	print '[comm: "%s"]' % comm
	os.environ["CHPL_COMM"] = comm
	os.environ["CHPL_GASNET_SEGMENT"] = chpl_comm_segment.get()
	os.environ["CHPL_LAUNCHER"] = launcher

	print '[localeModel: "%s"]' % locMod
	os.environ["CHPL_LOCALE_MODEL"] = locMod

	# skip stdin tests for most custom launchers, except for amdprun and slurm
	if (launcher != "none" and launcher != "amudprun" and launcher !=
		"slurm-srun" and not os.environ.get("CHPL_NO_STDIN_REDIRECT") 
		and not os.environ.get("CHPL_TEST_FORCE_STDIN_REDIRECT")):
		print ("[Info: assuming stdin redirection is not supported, skipping "
			"tests with stdin]")
		os.environ["CHPL_NO_STDIN_REDIRECT"] = "true"
	
	# launcher timeout
	if (not os.environ.get("CHPL_LAUNCHER_TIMEOUT") 
		and not os.environ.get("CHPL_TEST_DONT_SET_LAUNCHER_TIMEOUT")):
		if launcher.find("slurm") > 0:
			os.environ["CHPL_LAUNCHER_TIMEOUT"] = "slurm"
		if launcher.find("pbs") > 0 or launcher.find("qsub") > 0 :
			os.environ["CHPL_LAUNCHER_TIMEOUT"] = "pbs"

	# comm
	if comm != "none" and args.numlocales == "0" :
		args.numlocales = 1
	
	if args.numlocales == "0" :
		print '[numlocales: "(default)"]'
	else:
		print '[numlocales: "%s"]' % args.numlocales
	
	os.environ["NUMLOCALES"] = str(args.numlocales)

	# pre-exec
	if args.preexec:
		if os.path.isfile(args.preexec) and os.access(args.preexec, os.X_OK):
			args.preexec = os.path.abspath(args.preexec)
			print "[system-wide preexec: %s]" % args.preexec
		else:
			print ("[Error: Cannot find or execute system-wide preexec: %s" %
				args.preexec)
			finish()
		os.environ["CHPL_SYSTEM_PREEXEC"] = args.preexec

	# pre-diff
	if args.prediff:
		if os.path.isfile(args.prediff) and os.access(args.prediff, os.X_OK):
			args.prediff = os.path.abspath(args.prediff)
			print "[system-wide prediff: %s]" % args.prediff
		else:
			print ("[Error: Cannot find or execute system-wide prediff: %s" %
				args.prediff)
			finish()
		os.environ["CHPL_SYSTEM_PREDIFF"] = args.prediff


def auto_generate_tests():
	path = os.getcwd()
	with cd(home):
		if path == home or path == testdir and not args.cleanonly:
			if args.performance:
				# track the number of examples being tested
				print ("[Generating tests from the Chapel spec in %s/spec]" 
					% home)
				tempfilepath = os.path.join(home, "test/spectests.exec.out.tmp")
				tempfile = open(tempfilepath, "w")
				p = subprocess.Popen(["make", "spectests"], 
					stdout=subprocess.PIPE)
				tempfile.write(p.stdout.read())
				p.wait()
				tempfile.close()
				if p.returncode != 0:
					print ("[Error: Failed to generate Spec tests.  "
						"Log file: %s]") % tempfilepath
					finish()

				with cd(testdir):
					if not os.path.isdir(perfdir):
						os.mkdir(prefdir)

					print "[Computing stats for spec examples]"
					tempperfpath = os.path.join(home,
						"test/spectests.perfStats.out.tmp")
					tempperffile = open(tempperfpath, "w")
					cmd = os.path.join(utildir, "test/computePerfStats")
					p = subprocess.Popen([cmd, "spectests", perfdir, 
						os.path.join(home, "test/spectests.perfkeys"), 
						tempfilepath],stdout = subprocess.PIPE)
					tempperffile.write(p.stdout.read())
					p.wait()
					tempperffile.close()
					if p.returncode != 0:
						print ("[Error: Failed to compute perf stats for Spec"
								"tests. Log file: %s]" % tempperfpath)
						finish()

					os.remove(tempfilepath)
					os.remove(tempperfpath)

			elif not args.gengraphs: 		
				print "[Generating tests from the Chapel Spec in %s/spec" % home
				p = subprocess.Popen(["make", "spectests"], 
					stdout=subprocess.PIPE)
				autogen = p.stdout.read()
				if p.returncode != 0:
					print ("[Error: Failed to generate Spec tests. Run 'make"
							"spectests' in %s for more info]" % home)
					finish()


def print_chapel_environment():
	print
	print "### Chapel Environment ###"
	cmd = os.path.join(utildir, "printchplenv")
	p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
	printout(p.stdout)
	p.wait()
	print "##########################"


def check_for_duplicates():
	# check for .dat duplicates
	if args.performance:
		print "[Checking for duplicate performance data filenames]"
		
		dat_files = []
		error = False
		for root, dirnames, filenames in os.walk(testdir):
			for filename in fnmatch.filter(filenames, "*." + args.perflabel):
				if filename in dat_files: # duplicate
					print ("[Error: Duplicate performance data filenames ("
							"%s)" % filename)
					error = True
				else:
					dat_files.append(filename)

		if error:
			finish()
	
	# check for .graph files, and GRAPHFILES
	if args.gengraphs or args.compperformance:
		print ("[Checking for duplicate .graph files and that all .graph files "
				"appear in %s/test/.*GRAPHFILES]" % home)

		# find GRAPHFILES
		graphfiles = [f for f in os.listdir(testdir) if
				f.endswith("GRAPHFILES")]

		# read .graph files from GRAPHFILES
		graphfiles_graph_files = []
		for graphfile in graphfiles:
			f = open(os.path.join(testdir, graphfile), "r")
			for line in f:
				if line == "": continue
				if line.strip() == "" or line.strip()[0] == "#": continue
				graphfiles_graph_files.append(line.rstrip())
			f.close()

		# get absolute paths of actual .graph files
		actual_graph_files = []
		for root, dirnames, filenames in os.walk(testdir):
			for filename in fnmatch.filter(filenames, "*.graph"):
				actual_graph_files.append(os.path.relpath(
					os.path.join(root, filename), testdir))

		# check that actual .graph files are listed in GRAPHFILES
		for g in actual_graph_files:
			filename = g
			if filename not in graphfiles_graph_files:
				print "[Warning: %s is missing from GRAPHFILES]" % filename

		# check that all .graph files in GRAPHFILES actually exist
		for g in graphfiles_graph_files:
			filename = g
			if filename not in actual_graph_files:
				print ("[Warning: %s listed in GRAPHFILES does not exist]" %
						filename)

		# check for duplicates
		graphset = {}
		for g in actual_graph_files:
			filename = os.path.basename(g).lower()
			if filename in graphset:
				print ("[Warning: graph files must have unique, case "
						"insensitive names: %s and %s do not]" %
						(g, graphset[filename]))
			else:
				graphset[filename] = g


# END STUFF

def cleanup():
	if os.path.isdir(chpltesttmpdir):
		print "[Removing %s directory]" % chpltesttmpdir
		shutil.rmtree(chpltesttmpdir)


def jUnit():
	junit_args = "--start-test-log=%s" % logfile
	if args.junit_xml_file:
		junit_args += " --junit-xml=%s" % args.junit_xml_file
	
	if args.junit_remove_prefix:
		junit_args += " --remove-prefix=%s" % args.junit_remove_prefix
	elif args.testrootdir:
		# if --test-root was thrown, remove it from junit report
		junit_args += " --remove-prefix=%s" % args.testrootdir

	cmd = os.path.join(utildir, "test/convert_start_test_log_to_junit_xml.py")
	return getOutput([cmd, junit_args])


# PARSER

def parser_setup():
	parser = argparse.ArgumentParser(description="Test Chapel code.")
	# main args
	parser.add_argument("tests", nargs="*", help="test files or directories")
	# executing options
	parser.add_argument("-execopts", "--execopts", action="store_true",	
			dest="execopts", help="set options for executing tests")
	# compiler
	parser.add_argument("-compiler", "--compiler", action="store",
			dest="compiler", help="set alternate compiler")
	# program launch utility
	parser.add_argument("-launchcmd", "--launchcmd", action="store",
			dest="launchcmd", default="",
			help="set a program to launch generated executables")
	# compiler options
	parser.add_argument("-compopts", "--compopts", action="store_true",
			dest="compopts", help="set options for the compiler")
	# logfile
	parser.add_argument("-logfile", "--logfile", action="store",
			dest="logfile", help="set alternate logfile")
	# mem leaks log
	parser.add_argument("-memleaks", "--memleaks", action="store",
			dest="memleakslog", help=("set location for memLeaks log, "
				"and run with the --memLeaksLog flag") )
	# valgrind
	parser.add_argument("-valgrind", "--valgrind", action="store_true",
			dest="valgrind", help="run everything using valgrind")
	# pre exec, pre- etc....
	parser.add_argument("-syspreexec", "--syspreexec", action="store",
			dest="preexec", help="set a PREEXEC script to run before execution")
	parser.add_argument("-sysprediff", "--sysprediff", action="store",
			dest="prediff", help="set a PREDIFF script to run on test output")
	# future mode args
	futuresMode = 0
	parser.add_argument("-futures", "--futures", action="store_const", const=1,
			default=futuresMode, dest="futuresMode", help="run future tests")
	parser.add_argument("-futuresonly", "--futuresonly", "-futures-only",
			"--futures-only", action="store_const", const=2, default=
			futuresMode, dest="futuresMode", help="only run future tests")
	parser.add_argument("-futuresskipif", "--futuresskipif", "-futures-skipif",
			"--futures-skipif", action="store_const", const=3, default=
			futuresMode, dest="futuresMode", help="run future-skipif tests")
	parser.add_argument("-futures-mode", "--futures-mode", action="store", 
			default=futuresMode, dest="futuresMode",
			help=argparse.SUPPRESS)
	# cleaning
	parser.add_argument("-clean-only", "-cleanonly", "--clean-only",
			"--cleanonly", action="store_true", dest="cleanonly", 
			help="only clean files specified in CLEANFILES")
	# recurse
	parser.add_argument("-norecurse", "-no-recurse", "--norecurse",
			"--no-recurse", action="store_false", dest="recurse", 
			help="don't recurse down through directories")
	# performance 
	parser.add_argument("-performance", "--performance", action="store_true",
			dest="performance", help="run performance tests")
	parser.add_argument("-perflabel", "--perflabel", action="store", 
			default="perf", dest="perflabel", 
			help="set alternate performance test label")
	parser.add_argument("-performance-description",
			"--performance-description", action="store", default="",
			dest="performancedescription")
	parser.add_argument("-performance-configs", "--performance-configs",
			"-performance-configurations", "--performance-configurations",
			action="store", dest="performanceconfigs", 
			help="set performance configurations")
	parser.add_argument("-compperformance", "--compperformance",
			action="store_true", dest="compperformance",
			help="test compiler performance")
	parser.add_argument("-compperformance-description",
			"--compperformance-description", action="store", default="",
			dest="compperformancedescription")
	parser.add_argument("-numtrials", "--numtrials", "-num-trials",
		"--num-trials", action="store", dest="numtrials", default="1", 
		help="the number of times to run the performance tests")
	# graphing
	parser.add_argument("-gen-graphs", "--gen-graphs", "-generate-graphs",
			"--generate-graphs", action="store_true", dest="gengraphs",
			help="only generate graphs, don't run tests")
	parser.add_argument("-nodisplaygraphrange", "--nodisplaygraphrange",
			"-no-display-graph-range", "--no-display-graph-range",
			action="store_false", dest="graphsdisprange",
			help="don't display trial range envelopes on the graphs")
	parser.add_argument("-graphsgendefault", "--graphsgendefault",
			"-graphs-gen-default", "--graphs-gen-default", action="store",
			choices=["avg","min","max","med"], default="avg",
			dest="graphsgendefault", 
			help="set default method to reduce multiple trials")
	parser.add_argument("-startdate", "--startdate", action="store",
			dest="startdate", metavar="<MM/DD/YY>", help="set graph start date")
	parser.add_argument("-gengraphopts", "--gengraphopts", "-genGraphOpts", 
			"--genGraphOpts", action="store", dest="gengraphopts",
			help="set additional graph generation options")
	# only compile
	parser.add_argument("-comp-only", "--comp-only", action="store_true",
			dest="componly", help="only compile the tests, don't run")
	# valgrind exe
	parser.add_argument("-valgrindexe", "--valgrindexe", action="store_true",
			dest="valgrindexe", help="execute tests using valgrind")
	# num locales
	parser.add_argument("-numlocales", "--numlocales", action="store",
			dest="numlocales", default="0", 
	   		help="set the number of locales to run on")
	# stdin redirect
	parser.add_argument("-nostdinredirect", "--nostdinredirect",
			action="store", dest="nostdinredirect", 
			help="run the tests without redirecting stdin from /dev/null")
	parser.add_argument("-stdinredirect", "--stdinredirect", action="store",
			dest="stdinredirect", 
			help="force stdin redirection from /dev/null")
	# launcher timeout
	parser.add_argument("-launchertimeout", "--launchertimeout", action="store",
			dest="launchertimeout", 
			help="rely on the luancher to enforce the timeout, not timedexec")
	# no chpl home warning
	parser.add_argument("-no-chpl-home-warn", "--no-chpl-home-warn",
			action="store_false", dest="chplhomewarn",
			help="don't warn about not starting in CHPL_HOME")
	# progress
	parser.add_argument("-progress", "--progress", action="store_true",
			dest="progress", help="Log pass/fail for each test to stderr")
	# test root
	parser.add_argument("-test-root", "--test-root", action="store",
		dest="testrootdir", help="set absolute path to test directory")
	# jUnit
	parser.add_argument("-junit-xml", "--junit-xml", action="store_true",
			dest="junit_xml", help="create jUnit XML report")
	parser.add_argument("-junit-xml-file", "--junit-xml-file", action="store",
			dest="junit_xml_file", metavar="<file>",
			help="set the path to store the jUnit XML report")
	parser.add_argument("-junit-remove-prefix", "--junit-remove-prefix", 
			action="store", dest="junit_remove_prefix", metavar="<prefix>",
			help="set the <prefix> to remove from tests in the jUnit report")
	# extra help
	parser.add_argument("-help", action="help", help=argparse.SUPPRESS)

	return parser 


def parse_passthrough_opts():
	if args.compopts:
		getting = False
		for arg in sys.argv:
			if not "-compopts" in arg and not getting: continue
			if not getting:
				getting = True
				args.compopts = []
				continue
			if not arg[0:2] == "--":
				break
			args.compopts.append(arg)
		args.compopts = " ".join(args.compopts)
	if args.execopts:
		getting = False
		for arg in sys.argv:
			if not "-execopts" in arg and not getting: continue
			if not getting:
				getting = True
				args.execopts = []
				continue
			if not arg[0:2] == "--":
				break
			args.execopts.append(arg)
		args.execopts = " ".join(args.execopts)
	else:
		args.execopts = ""


# UTILITY ROUTINES AND CLASSES

class Logger(object):
	def __init__(self):
		self.terminal = sys.stdout
		self.log = open(logfile, "a")
	def write(self, message):
		self.terminal.write(message)
		self.log.write(message)
	def flush(self):
		self.terminal.flush()
	 	self.log.flush()

def printout(so):
	while True:
		line = so.readline()
		if not line: break
		sys.stdout.write(line)
 		sys.stdout.flush()
 
def getOutput(cmd):
	p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
	p.wait()
	return p.stdout.readline().rstrip()

@contextlib.contextmanager
def cd(path):
	old_dir = os.getcwd()
	os.chdir(path)
	os.environ["PWD"] = path
	try: yield
	finally:
		os.chdir(old_dir)
		os.environ["PWD"] = old_dir


try:
	main()
except KeyboardInterrupt:
	sys.exit(1);
