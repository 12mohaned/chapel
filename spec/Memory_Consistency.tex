\sekshun{Memory Consistency Model}
\label{Memory_Consistency_Model}
\index{memory consistency model}

\begin{openissue}
  This chapter is a work-in-progress and represents an area where we
  are particularly interested in feedback from, and collaboration
  with, the broader community.
\end{openissue}

In this section, we describe Chapel's memory consistency model. This memory
consistency model is in spirit of the Sequential Consistency for Data Race free
programs model adopted by C11, C++11, Java, UPC, and Fortran 2008. In
particular, correct programs will use synchronization constructs when tasks
communicate with each other. The model does not define the behavior of
incorrect programs with data races.

The \chpl{forall}, \chpl{coforall}, \chpl{cobegin}, \chpl{begin}, as well as
the square-bracket notation and promoted functions or operators can create
multiple tasks in a Chapel program. When more than one of these tasks could
operate on the same memory at the same time, and at least one of the operations
is a write, there will be a \textit{data race} unless the tasks are correctly
synchronized.

Chapel has many features that can be used to correctly synchronize tasks:

\begin{itemize}

  \item fork-join parallelism with \chpl{forall}, \chpl{coforall},
\chpl{cobegin}, or with \chpl{begin} and \chpl{sync} statements

  \item shared variables safe for multiple tasks to access, including
\chpl{sync}, \chpl{single}, and \chpl{atomic} variables

\end{itemize}

\section{Design Principles}

We used the following design principles in developing the memory consistency
model for Chapel.

1. Sequential programs have program order semantics. Programs that are totally
   sequential cannot have data races and should execute as though each statement
   was executed one at a time and in the natural order.

2. Chapel's fork-join constructs introduce additional order dependencies.
   Operations within a task cannot behave as though they started before the
   task started. Similarly, all operations in a task must appear to a parent
   task to be completed when the parent task joins with that task.

3. Distributed memory programs have the same memory consistency model as local
   programs. The Chapel language seeks to allow a single description of an
   algorithm to work with different data distributions. A result of this
   property is that an expression of a program must be correct whether it is
   working on local or distributed data.

4. Chapel's memory model should be as relaxed as possible to achieve the above
   design principles. In particular, making all operations provide sequential
   consistency is not likely to enable good performance. At the same time,
   sequential consistency should be available to programmers when requested.

See \textit{A Primer on Memory Consistency and Cache Coherence} by Sorin,
Daniel J. and Hill, Mark D. and Wood, David A. for more background information
on memory consistency models. This section will proceed in a manner inspired by
the $XC$ memory model described there.

\section{Overview of the Memory Consistency Model}

The memory consistency model for Chapel is the same as the C11 and C++11 memory
consistency models, with some minor exceptions which are described in section
\ref{relating_to_C_MCM}. Section \ref{SC_for_DRF} directly describes Sequential
Consistency for Data Race Free programs as an alternative way of understanding
the recommended subset of the C11 and C++11 memory consistency models. This
description does not fully specify the behavior of atomics using a more relaxed
ordering than sequential consistency. While those features are available in
Chapel, they are not recommended casual use since it difficult to understand
how to write correct programs. Section \ref{relating_to_C_MCM} describes how to
use the C11 and C++11 specifications to understand the behavior of Chapel
programs using these more relaxed features.

\section{Sequential Consistency for Data Race Free Programs}
\label{SC_for_DRF}
\index{memory consistency model!sequential consistency for data race free programs}

The memory consistency model is described in terms of two orders:
\textit{program order} and \textit{memory order}. The \textit{program order}
$<_p$ is a partial order describing serial or fork-join parallelism
dependencies between variable reads and writes. The \textit{memory order} $<_m$
is a total order that describes the semantics of atomic operations with
sequential consistency (SC). Other forms of atomic operations do not create
this total order and will be discussed in the next section.

We will use the following notation:
\begin{itemize}

  \item $L(a)$ indicates a \textit{load} from a variable at address $a$.  $a$
could point to local or remote memory.

  \item $S(a)$ indicates a \textit{store} to a variable at address $a$.  $a$
could point to local or remote memory.

  \item $A(a,o)$ indicates an \textit{atomic operation} on a variable at
address $a$ with ordering constraint $o$. As with \textit{load} and
\textit{store}, $a$ can point to local or remote memory. $o$ can be relaxed,
acquire, release, or sequential consistency (SC). These atomic operations must
be completed at once (ie atomically) and so it is not possible to read an
intemediate state from an atomic operation, even with relaxed consistency. 

  \item $A(a)$ indicates an \textit{atomic operation} on a variable at address
$a$ with sequential consistency (SC) ordering constraint

  \item $L(a)$, $S(a)$, and $A(a,o)$ are also called \textit{memory operations}
  \item $X <_p Y$ indicates that $X$ preceeds $Y$ in program order
  \item $X <_m Y$ indicates that $X$ preceeds $Y$ in memory order
  \item \chpl{t = begin\{X\}} start a new task named $t$ to execute $X$
  \item \chpl{wait($t_1$..$t_n$)} wait for tasks $t_1..t_n$ to complete
  \item $on$ $L$ run the remainder of this task on another locale $L$
\end{itemize}

0. Task creation and task waiting create a conceptual tree of program
statements. Thus, the body of the tasks, task creation, and task wait
operations create a partial order $<_p$ on memory operations. We will call
$<_p$ \textit{program order}. For the purposes of this section, the statements
in the body of each Chapel task will be implemented in terms of \textit{load},
\textit{store}, and \textit{atomic operation}. The $on$ statement changes where
the current task is running but otherwise has no impact on the memory
consistency requirements.

\begin{itemize}

  \item If we have a program snippet without tasks, such as \chpl{X; Y;}, where
$X$ and $Y$ are memory operations (ie one of $L(a)$,$S(a)$, or $A(a,o)$), then
$X <_p Y$.

  \item The program \chpl{X; begin\{Y\}; Z;} implies $X$ $<_p$ $Y$ but there is
no particular relationship between $Y$ and $Z$ in program order.

  \item The program \chpl{t = begin\{Y\}; wait(ti); Z;} implies $Y$ $<_p$ $Z$
  \item $X$ $<_p$ $Y$ and $Y$ $<_p$ $Z$ imply $X$ $<_p$ $Z$
\end{itemize}

For the purposes of describing this memory model, it is assumed that Chapel
programs will be translated into sequences of \textit{memory operations},
\chpl{begin}, and \chpl{wait} statements. The translation of a Chapel program
into a sequence of \textit{memory operations} must preserve sequential program
semantics. That is, if we have a snippet of a a Chapel program without task
operations, such as \chpl{X; Y;}, the statements $X$ and $Y$ will be converted
into a sequence of \textit{load}, \textit{store}, and \textit{atomic
operations} in a manner that preserves the behavior of a serial portion of the
program. Operations in one statement preceed operations in the next statement.
Thus, $X=x_1,x_2,...$ and $Y=y_1,y_2,...$ where $x_i$ and $y_j$ are each a
sequence of \textit{load}, \textit{store}, or \textit{atomic operations} and we
have $x_i <_p y_j$.

Likewise, for the purposes of this memory model, Chapel's parallelism keywords
are viewed as a sequence of operations including the primitives of starting a
task (\chpl{begin}) and waiting for some number of tasks
(\chpl{wait(ti..tn)}). In particular:

\begin{itemize}

  \item \chpl{forall} creates some implementation-dependent number of tasks $m$
with \chpl{$t_i$ = begin\{some-loop-bodies\}} to execute some number of loop
iterations and waits for them to complete with \chpl{wait($t_1$..$t_m$)}.

  \item \chpl{coforall} creates one task per loop iteration (\chpl{$t_i$ =
begin\{loop-body\}} for all loop iterations $i=1..n$) and then waits for them
all to complete (with \chpl{wait($t_1$..$t_n$)}).

  \item \chpl{cobegin} creates one task per contained statement (\chpl{$t_i$ =
begin\{$X_i$\}} for all contained statements $X_i$ with $i=1..n$) and then
waits for them all to complete (with \chpl{wait($t_1$..$t_n$)}).

  \item \chpl{begin} creates a task to execute the contained statement and adds
it to a list of tasks executing in dynamic scope (\chpl{t = begin\{X\}}).

  \item \chpl{sync} statement waits for any of the $n$ tasks $t_i$ created in
the dynamic scope of the sync statement with \chpl{wait($t_1$..$t_n$)}.

\end{itemize}

For the purposes of the memory model, the \chpl{on} statement does not have any
special meaning for program order. In other words, replacing the \chpl{on}
statement with a block statement would result in a program with equivalent
program order. Put another way, the \chpl{on} statement has sequential
semantics and the memory consistency model is the same no matter where a task
is executing.

Also note that \chpl{sync} variables will have memory consistency behavior
equivalent to a sequence of operations on \chpl{atomic} variables and so are
not separately described here. In particular, for the purposes of memory
consistency, a operations on a \chpl{sync} variable will have the same
consistency implications as if an \chpl{atomic} variable were used with
SC as a lock to protect modifications of the \chpl{sync}
variable's value or full/empty state.


1. All tasks insert SC atomic operations into the order $<_m$ respecting
program order as described in the rules below. Note that atomic operations with
relaxed semantics (relaxed, acquire, or release semantics which are less strict
than SC) do not create a total order. We will discuss relaxed atomic operations
below.

\begin{itemize}
  \item If $A(a)<_pA(b)$ then $A(a)<_mA(b)$
\end{itemize}

2. Every SC atomic operation gets its value from the last SC atomic operation before it to the same address in the total order $<_m$:
\begin{itemize}
  \item Value of $A(a)$ = Value of $MAX_{<_m} \{A'(a)|A'(a) <_m A(a) \}$
\end{itemize}

3. For data race free programs, all tasks insert loads and stores into the
total order $<_m$ respecting the below rules which preserve the order of loads
and stores relative to atomic operations.  Note that if a program contains data
races, the \textit{memory order} is not an order at all for the racy loads
$L(a)$ and stores $S(a)$ because these loads and stores do not have to be
completed at once (in other words, two racing stores to the same address could
result in a value being written that is neither of the written values but is a
combination of the two writes). Below we formalize the order in which data race free programs insert loads and stores into the total memory order:
\begin{itemize}
  \item If $L(a)<_pA(b)$ then $L(a)<_mA(b)$
  \item If $S(a)<_pA(b)$ then $S(a)<_mA(b)$
  \item If $A(a)<_pL(b)$ then $A(a)<_mL(b)$
  \item If $A(a)<_pS(b)$ then $A(a)<_mS(b)$
\end{itemize}

4. For data race free programs, all tasks insert their loads and stores to the
same address into the order $<_m$ respecting the following rules which preserve
sequential program semantics:

\begin{itemize}
  \item If $L(a) <_p L'(a)$ then $L(a) <_m L'(a)$
  \item If $L(a) <_p S(a)$ then $L(a) <_m S(a)$
  \item If $S(a) <_p S'(a)$ then $S(a) <_m S'(a)$
\end{itemize}

5. For data race free programs, every load gets its value from the last store before it to the same address in the total order $<_m$:
\begin{itemize}
  \item Value of $L(a)$ = Value of $MAX_{<_m}$ \{ $S(a)|S(a)$ $<_m$ $L(a)$ or $S(a)$ $<_p$ $L(a)$ \}
\end{itemize}

% This table is commented out because I'm not totally sure it's correct.
% Or at least it might be misleading. In particular, I don't feel that
% this conveys that transformations are allowed only if they wouldn't
% change sequential program behavior. In particular, if an address
% is computed based on the result of one load, the second load can't go
% before the first load.
%
% B = load A
% load B
%
% these loads are to different addresses but the order must be preserved.
%
% The table below describes which operations can be reordered with other
% operations in the translation from program order to the global order.  The rows
% indicate the first operation and the columns indicate the second operation. An
% X indicates that the ordering is always enforced. An A indicates that the
% ordering is only enforced if the operations are to the same address.
% 
% \begin{center}
% \begin{tabular}{|r|r|r|r|}
% \hline
%        & Load & Store & Atomic \\ \hline
% Load   &    A &     A &      X \\ \hline
% Store  &    A &     A &      X \\ \hline
% Atomic &    X &     X &      X \\ \hline
% \end{tabular}
% \end{center}

\section{Data Structure Consistency}

Generally speaking, data structures provided by the Chapel language and
libraries are not safe for concurrent use when one of those uses is
modification. In particular, 

\begin{itemize}

\item Elements of a Chapel array are not safe against \textit{data races}
unless they are of an \textit{atomic} or \textit{sync} type.

\item Chapel arrays and domains are not safe for concurrent changes to
their shape.

\item Chapel arrays are safe for concurrent updates to different elements
that already exist or for read-only access from multiple tasks.

\end{itemize}

Data structure authors should make an effort to document how their data
structure is safe or is not safe for concurrent updates. Generally speaking,
data structures might fall into these categories:

\begin{itemize}

\item safe for any concurrent updates (e.g. a work queue)

\item safe for concurrent updates to disjoint regions but not for shape
changes (e.g. an array)

\item not safe for any concurrent updates, even to disjoint regions (e.g.
the a string that reallocates itself when a character is changed)

\end{itemize}

The data structure consistency could operate with either of these
strategies:

\begin{itemize}

\item data structure updates will be visible once the usual task
synchronization takes place, or

\item the data structure requires consistency to be managed explicitly
(e.g. with a 'flush' call).

\end{itemize}

\section{Relationship to the C11 and C++11 memory models}
\label{relating_to_C_MCM}
\index{memory consistency model!relationship to C memory consistency model}

We have already shown an outline of the memory consistency model for Chapel.
Some features of the language allow you to request weaker than SC ordering.
This section describes how to understand Chapel programs in terms of the C11
and C++11 specifications. We will summarize some of the features of the C11 and
C++11 memory models. These summaries should be taken only as an aid in
understanding. Also, we will refer to the C++ model specifically but keep
in mind that the C memory model is substantially similar.

\subsection{Understanding the C++11 memory model}

The C++11 specification describes the memory consistency model in terms of
a total order on atomic operations \textit{separately for each atomic
memory location}. It is does \textit{not} guarantee a global total order
on atomic operations unless the SC consistency is always used. In
particular, for relaxed consistency atomics, the operations on the atomics
do complete in a total order \textit{for each atomic memory location}.  In
other words, at any given moment, an atomic at address $a$ updated with a
relaxed atomic operation by any number of threads will either have the
result of the update or not. It cannot have an intermediate state.
However, since it is a total order for each address, relaxed atomic
operations to two different memory locations could be reordered.

The C++11 memory models also use some key terms, which we will summarize here:

\begin{itemize}

  \item \textit{thread} a execution context for a stream of instructions.

  \item \textit{acquire semantics} means that a memory operation cannot move
  from after the acquire operation to before the acquire operation. That means,
  for example, that a load must return the current value and not a value cached
  from before the acquire operation.

  \item \textit{release semantics} means that a a memory operation cannot move
  from before the release operation to after the release operation. That means,
  for example, that a previously started store must complete before the release
  operation completes.

  \item \textit{synchronizes with} expresses one how threaded programs need
  memory operations to be ordered. A \textit{release} operation to a particular
  memory location M \textit{synchronizes with} an \textit{acquire} operation
  that takes its value from the \textit{release} operation. In addition, thread
  creation operation synchronizes with the new thread; and the thread
  completion synchronizes with the joining operation in the parent thread.

  \item \textit{sequenced before} is a relationship indicating which operations
  must always occur before other operations within the same thread in order to
  preserve sequential program semantics.

  \item \textit{happens before} is a relationship indicating which operations
  must always occur before other operations according to the memory consistency
  model. \textit{happens before} includes the ordering required by
  \textit{synchronizes with} as well as the ordering required by
  \textit{sequenced before}.

\end{itemize}
 
The C++11 specifications provides the following memory orders that are also
available in Chapel that can be used to request particular memory consistency
for each atomic operation:

% C standard section 5.1.2.4 p 20, 7.17.3 and 7.17.4 p 277. C++ standard section 1.10 p11 and and 29 p 1113

\
\begin{itemize}

  \item \chpl{memory\_order\_relaxed} For an atomic operation, the value of the
  atomic variable itself will be updated in a single operation, but the order
  of other loads and stores or the order of relaxed atomic operations to other
  addresses is not necessarily preserved.

  \item \chpl{memory\_order\_acq\_rel} a store operation has \textit{release
  semantics} and a load operation has \textit{acquire semantics}.

  \item \chpl{memory\_order\_acquire} a load operation has \textit{acquire
  semantics}.

  \item \chpl{memory\_order\_release} a store operation has \textit{release
  semantics}.

  \item \chpl{memory\_order\_seq\_cst} Includes the properties of both
  \chpl{memory\_order\_acquire} and \chpl{memory\_order\_release} and
  additionally requires that there be a total ordering on
  memory\_order\_seq\_cst operations. This is the default and corresponds
  to SC ordering in section \ref{SC_for_DRF} above.

\end{itemize}

\subsection{Mapping Chapel concepts to the C++11 memory model}

The Chapel memory model is a generalization of the C++ memory model to
distributed memory. To understand the semantics of programs using non SC
atomics, it is necessary to refer to the C++ memory model. Chapel programs
will follow the C++ memory model with the following constraints:

\begin{itemize}

  \item a \textit{task} in Chapel corresponds to a \textit{thread} in C++
  for the purposes of the memory model

  \item Chapel's parallel language features correspond to some combination of
  thread creation and thread joining in the C++11 memory model. In particular,
  the language features are described above in terms of \chpl{begin} and
  \chpl{wait}. \chpl{begin} corresponds to thread creation and \chpl{wait}
  corresponds to thread joining.

  \item Chapel's \chpl{sync} variables are equivalent to locks in the
  C++11 memory model.

  \item The idea of \textit{program order} described above replaces the
  C++11 notion of\textit{sequenced before}.

  \item the \chpl{on} statement has no effect on memory consistency.

  \item whether data is local or remote has no effect on memory consistency.

\end{itemize}

\section{Relaxed Atomic Operations}

Sequential consistency for atomic operations can be slow in some
circumstances. As a result, a programmer might be tempted to use relaxed
atomic operations by specifying the order \chpl{memory\_order\_relaxed}.
Such uses of atomic operatios must be done with care and should generally
not be used to synchronize tasks.

The least error prone ways to use \chpl{memory\_order\_relaxed} is for
variables that need atomic updates but that are not used to synchronize
tasks. An example would be a running total computed by multiple tasks.

It is just as unreasonable to synchronize tasks with a non-atomic variable
as it is to synchronize them with a \chpl{memory\_order\_relaxed} atomic
variable. Both of these are disasterous for program correctness and should
be avoided. Note that advanced users can combine
\chpl{memory\_order\_relaxed} with fences to synchronize tasks.

Although relaxed atomics do not complete in a total order by themselves,
and might contribute to non-deterministic programs, relaxed atomics cannot
contribute to a \textit{data race} that would prevent \textit{sequential
consistency}.

When relaxed atomics are used only for atomicity and not as part of
synchronizing tasks, their effect can be understood in the above memory
consistency model described in \ref{SC_for_DRF} by treating them as
ordinary loads or stores - with two exceptions:

\begin{itemize}

\item atomic operations (including relaxed atomic operations) cannot
create a \textit{data races}

\item all atomic operations (including relaxed atomic operations) should
eventually be visible to all other threads. This property is not true for
normal loads and stores. This property is paragraph 25 in the C++
specifiacion section 1.10. 

\end{itemize}

\section{Unordered Loads and Stores}

The Chapel memory model also supports the idea of \textit{unordered} loads
and stores.

Instead of doing normal loads and stores to read or write local or remote
memory, a Chapel program can use \textit{unordered} loads and stores when
preserving sequential program behavior is not important. Let's use this
notation:

\begin{itemize}

  \item $UL(a)$ indicates an \textit{unorderd} \textit{load} from a
  variable at address $a$. $a$ could point to local or remote memory.

  \item $US(a)$ indicates an \textit{unordered} \textit{store} to a
  variable at address $a$. Again, $a$ could point to local or remote
  memory.

\end{itemize}

The \textit{unordered} loads and stores $UL(a)$ and $US(a)$ respect fences
but not program order. Rule 3 from section \ref{SC_for_DRF} above is
extended to \textit{unordered} operations:

\begin{itemize}
  \item If $UL(a)<_pA(b)$ then $UL(a)<_mA(b)$
  \item If $US(a)<_pA(b)$ then $US(a)<_mA(b)$
  \item If $A(a)<_pUL(b)$ then $A(a)<_mUL(b)$
  \item If $A(a)<_pUS(b)$ then $A(a)<_mUS(b)$
\end{itemize}

but Rule 4 from section \ref{SC_for_DRF} above is not extended:

\begin{itemize}
  \item $UL(a) <_p UL'(a)$ does not imply $UL(a) <_m UL'(a)$
  \item $UL(a) <_p US(a)$ does not imply $UL(a) <_m US(a)$
  \item $US(a) <_p US'(a)$ does not imply $US(a) <_m US'(a)$
\end{itemize}

\textit{unordered} operations should be thought of as happening in a way that
is overlapped with the program task. \textit{unordered} operations started in
different program statements happen at the same time unless there is a memory
fence between them.

Since \textit{unordered} operations started by a single task can happen at
the same time, totally sequential programs can have a \textit{data race}
when using \textit{unordered} operations. This follows from our original
definition of \textit{data race}.

\begin{chapel}
var x: int;
x.unordered_store(10);
x.unordered_store(20);
writeln(x);
\end{chapel}

The value of \textit{x} at the end of this program could be 10 or 20. As a
result, programs using \textit{unordered} loads and stores are not sequentially
consistent unless the program can guarantee that \textit{unordered} operations
can never operate on the same memory at the same time when one of them is a
store. In particular, the following are safe:

\begin{itemize}
  \item \textit{unordered} stores to disjoint regions of memory
  \item \textit{unordered} loads from possibly overlapping regions of memory when no store could overlap with the loads
  \item \textit{unordered} loads and stores to the same memory location
  when these are always separated by a fence, synchronization variable, or
  atomic operation.
\end{itemize}

\textit{unordered} loads and stores are available as a performance
optimization. For example, a program computing a permutation on an array might
want to move data between two arrays without requiring any ordering:

\begin{chapel}
const n = 10;
// P is a permutation on 1..n, in this case reversing its input
var P = for i in 1..n by -1 do i;
var A = for i in 1..n do i;
// Compute the permutation applied to A
var B:[1..n] int;

for i in 1..n {
  ref dst = B[P[i]];
  dst.unordered_store(A[i]);
}
\end{chapel}

\begin{openissue}
  We have not finalized syntax for \textit{unordered} operations.

  Should there be memory fences that only complete \textit{unordered} operations started by the current task?

  Should \textit{unordered} operations on a particular address always wait for the address to be computed?

  Does starting a task or joining with a task wait for unordered
  operations to complete? (It would appear so from the current description
  and the C++11 semantics where thread creation and joining create a
  \textit{synchronizes with} relationship).
\end{openissue}

\section{Discussion}

Chapel's memory model does not include the \textit{write atomicity} or
\textit{store atomicity} property for general variable writes for two reasons.
First, an RDMA message could be partway through copying some data when another
thread reads that data. Second, two portions of the machine could have some
local hardware caches that complete writes for other cores at different times
from further away cores as allowed by the C11 or C++11 memory models. Sorin,
Hill and Wood define \textit{write atomicity} as the property that a store
operation is logically seen by all other cores at once (section 5.5.2). This
property is upheld for SC operations on \chpl{atomic} variables by
construction (since these are in a total order). However, \textit{write
atomicity} is not upheld for all atomic operations. For a given moment in any
task's execution, a write to any \chpl{atomic} variable write is either
entirely completed or entirely not yet started. This property is guaranteed for
any \chpl{atomic} variables with any ordering constraint including relaxed.
However, the property that a given write is seen by all other tasks at once is
not upheld for atomic operations unless the SC ordering is used.

For a distributed memory system, the straightforward implementation of SC
atomics or sync variables will be sequentially consistent. The straightforward
implementation is to follow these rules:

\begin{itemize}

 \item every task issues these SC atomic operations in program order

 \item each SC atomic operation is performed only at the 'home' of the atomic
 variable in question (e.g. with an Active Message or with network hardware
 support)

 \item each task does not start another operation until the SC atomic operation
 it is working on has completed on the 'home' of the atomic variable

\end{itemize}

In that case, the reasoning in Adve Hill "Implementing Sequential Consistency
in Cache-Based Systems" applies and the executions of these synchronization
operations will be sequentially consistent.

If the SC atomic operations themselves are sequentially consistent and the
program is free of data races, the load and store operations will also be
sequentially consistent. The reasoning here is that:

\begin{itemize}

 \item The load and store operations cannot be reordered across SC atomic
 operations (Rule 3 above).

 \item Access by two different tasks to the same memory location, when at
 least one of the accesses is a store, is a race condition. In order to
 avoid race conditions and preserve SC semantics, accesses to the same
 memory location from different tasks when one of the accesses is a store
 must be mediated by SC atomic operations.

 \item The SC atomic operation constrains the order of the load and store
 operations so that they appear as a total order.
\end{itemize}

Sketch of proof. Suppose that a data race free program produces a non-SC
outcome. That would mean that there is no total ordering on the loads or stores
to a particular memory location. If the memory location is never updated in the
relevant program region, the loads could be considered to be in any order in
the total order and provide the same result. If at least one of the operations
is a store, and it is not already constrained by the SC atomic operations, then
there is a race condition.

%(Corrolary 1: LLVM optimizations and C program optimizations are OK because C programs cannot reorder SC atomic ops)
%(Corralary 2: Cache is OK because:
% - any communication not mediated by atomics constitutes a data race
% - use of SC atomics causes cache flush/invalidate)

%Adve phd thesis
%Scheurich and Dubios ScD87 and Sch89 - sufficient condition for sequential consistency.
%Collier proved .... writes in same order equivalent to atomically... thus system ... is sequential consistenc. Col84-92 and DuS90.
%LHH91 also do it. All writes are seen in the same order by all processors -> sequentially consistent.
%\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.8176&rep=rep1&type=pdf}: An execution is consistent with respect to a consistency model... and these 3 invariants are sufficient to guarantee memory consistency defined in this way: Uniprocessor Ordering, Allowable Reordering, and Cache Coherence.
%i1. typical implementation is coherent:
% coherent "a coherent system must appear to execute all thread's loads and stores to a single memory location in a total order that respects the program order of each thread".
% Sufficient Conditions for SC:
%  - Every processor issues memory ops in program order
%  - Processor must wait for store to complete before issuing next memory operation
%  - After load, issuing proc waits for load to complete, and store that produced value to complete before issuing next op
%Scheurich Dubois and Brgiss 1988 Synchronization, Coherence, and Event Ordering in Multiprocessors.
%\url{ftp://ftp.cs.wisc.edu/markhill/Papers/icpp90_seqcon.pdf} sufficient conditions for SC including the above.
%\url{http://www.cs.berkeley.edu/~kubitron/cs252/lectures/lec20-sharedmemory3.pdf} has a picture of "exclusion zone" and argument for why processor atomics -> sequential consistency.

%\begin{chapelexample}{syncFenceFlag}
\begin{example}
  In this example, a synchronization variable is used to (a) ensure that
  all writes to an array of unsynchronized variables are complete, (b)
  signal that fact to a second task, and (c) pass along the number of
  values that are valid for reading.

  The program
\begin{chapel}
var A: [1..100] real;
var done(*\texttt{\$}*): sync int;           // initially empty
cobegin {
  { // Reader task
    const numToRead = done(*\texttt{\$}*);   // block until writes are complete
    for i in 1..numToRead do
      writeln("A[", i, "] = ", A[i]);
  }
  {  // Writer task
    const numToWrite = 23;     // an arbitrary number
    for i in 1..numToWrite do
      A[i] = i/10.0;
    done(*\texttt{\$}*) = numToWrite;        // fence writes to A and signal done
  }
}
\end{chapel}
  produces the output
\begin{chapelprintoutput}{}
A[1] = 0.1
A[2] = 0.2
A[3] = 0.3
A[4] = 0.4
A[5] = 0.5
A[6] = 0.6
A[7] = 0.7
A[8] = 0.8
A[9] = 0.9
A[10] = 1.0
A[11] = 1.1
A[12] = 1.2
A[13] = 1.3
A[14] = 1.4
A[15] = 1.5
A[16] = 1.6
A[17] = 1.7
A[18] = 1.8
A[19] = 1.9
A[20] = 2.0
A[21] = 2.1
A[22] = 2.2
A[23] = 2.3
\end{chapelprintoutput}
%\end{chapelexample}
\end{example}


\begin{chapelexample}{syncSpinWait.chpl}
One consequence of Chapel's memory consistency model is that a task cannot spin-wait on a
variable waiting for another task to write to that variable.  The behavior of
the following code is undefined:

\begin{chapelpre}
if false { // }
\end{chapelpre}
\begin{chapel}
var x: int;
cobegin with (ref x) {
  while x != 1 do ;  // spin wait
  x = 1;
}
\end{chapel}
\begin{chapelnoprint}
// {
}
\end{chapelnoprint}
In contrast, spinning on a synchronization variable has well-defined
behavior:
\begin{chapel}
var x(*\texttt{\$}*): sync int;
cobegin {
  while x(*\texttt{\$}*).readXX() != 1 do ;  // spin wait
  x(*\texttt{\$}*).writeXF(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

In this code, the first statement in the cobegin statement executes a
loop until the variable is set to one.  The second statement in the
cobegin statement sets the variable to one.  Neither of these
statements block.
\end{chapelexample}

\begin{chapelexample}{atomicSpinWait.chpl}
Atomic variables provide an alternative means to spin-wait. Atomic variables include a waitFor method that will block the calling thread until a read of the atomic value matches a particular value. For example:

\begin{chapel}
var x: atomic int;
cobegin with (ref x) {
  x.waitFor(1);
  x.write(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

\end{chapelexample}


\begin{future}
Upon completion, Chapel's atomic statement~(\rsec{Atomic_Statement}) will serve as
an additional means of correctly synchronizing between tasks.
\end{future}

