\sekshun{Memory Consistency Model}
\label{Memory_Consistency_Model}
\index{memory consistency model}

\begin{openissue}
  This chapter is a work-in-progress and represents an area where we
  are particularly interested in feedback from, and collaboration
  with, the broader community.
\end{openissue}

In this section, we describe Chapel's memory consistency model. This memory
consistency model is in spirit of the Sequantial Consistency for Data Race free
programs model adopted by C11, C++11, Java, and UPC. In particular, correct
programs will use synchronization constructs when tasks communicate with each
other. The model does not define the behavior of incorrect programs with data
races.

The \chpl{forall}, \chpl{coforall}, \chpl{cobegin}, \chpl{begin}, as well as
the square-bracket notation and promoted functions or operators can create
multiple tasks in a Chapel program. When more than one of these tasks operate
on the same memory, and at least one of the operations is a write, there will
be a data race unless the tasks are correctly synchronized.

Chapel has many features that can be used to correctly synchronize tasks:
\begin{itemize}
  \item fork-join parallelism with \chpl{forall}, \chpl{coforall}, \chpl{cobegin}, or with \chpl{begin} and \chpl{sync} statements
  \item shared variables safe for multiple tasks to access, including \chpl{sync}, \chpl{single}, and \chpl{atomic} variables
\end{itemize}

See \textit{A Primer on Memory Consistency and Cache Coherence} by Sorin,
Daniel J. and Hill, Mark D. and Wood, David A. for more background information
on memory consistency models. This section will proceed in a manner inspired by the $XC$ memory model described there.

%Also see C++11 \url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3797.pdf} and UPC 1.3 \url{https://upc-lang.org/assets/Uploads/spec/upc-lang-spec-1.3.pdf}  book \url{https://class.stanford.edu/c4x/Engineering/CS316/asset/A_Primer_on_Memory_Consistency_and_Coherence.pdf}
%p 58 XC defined
%p 65 SC for DRF defined
%Memory Orders

%\begin{itemize}
%  \item \chpl{memory\_order\_relaxed} For an atomic operation, the value of the atomic variable itself will be updated in a single operation, but the order of other loads and stores or the order of relaxed atomic operations is not necessarily preserved
%  \item \chpl{memory\_order\_acquire} After this operation, any loads cannot return a value read by this task before the \textit{acquire} operation.
%  \item \chpl{memory\_order\_release} Any stores occuring in this task must complete before this \textit{release} operation
%  \item \chpl{memory\_order\_seq\_cst} Includes the properties of both \chpl{memory\_order\_acquire} and \chpl{memory\_order\_release} and additionally requires that there be a total ordering on seq\_cst operations
%\end{itemize}
%
%\begin{itemize}
%  \item \textit{load} a read of a variable
%  \item \textit{store} a write to a variable
%  \item \textit{acquire} an acquire fence, available by request for an operation on a \chpl{atomic} variable  by specifying \chpl{memory\_order\_acquire} or with a call to \chpl{atomic\_fence(memory\_order\_acquire)}
%  \item \textit{release} a release fence, available by request for an operation on a \chpl{atomic} variable  by specifying \chpl{memory\_order\_release} or with a call to \chpl{atomic\_fence(memory\_order\_release)}
%  \item \textit{fence} a full fence, implied by operations on \chpl{atomic} or \chpl{sync} variables by default, and available with \chpl{atomic\_fence}. For the purpose of this discussion, \textit{fence} is equivalent to \textit{acquire} followed by \textit{release}.
%\end{itemize}
%
%When the same memory location is written by one task and read by another, the
%ordering of the read relative to the write is undefined unless there is a
%\textit{release} fence after the write and an \textit{acquire} fence on the
%read.

The memory consistency model is described in terms of two orders: \textit{program order} and \textit{memory order}. The \textit{program order} $<_p$ is a partial order describing serial or fork-join parallelism dependencies between variable reads and writes. The \textit{memory order} $<m$ is a total order that describes the semantics of atomic operations with sequential consistency (SC). Other forms of atomic operations will be discussed in the next section.

We will use the following notation:
\begin{itemize}
  \item $L(a)$ indicates a \textit{load} from a variable at address $a$
  \item $S(a)$ indicates a \textit{store} to a variable at address $a$
  \item $A(a,o)$ indicates an \textit{atomic operation} on a variable at address $a$ with ordering constraint $o$
  \item $A(a)$ indicates an \textit{atomic operation} on a variable at address $a$ with sequential consistency (SC) ordering constraint
  \item $L(a)$, $S(a)$, and $A(a,o)$ are also called \textit{memory operations}
  \item $X <_p Y$ indicates that $X$ preceeds $Y$ in program order
  \item $X <_m Y$ indicates that $X$ preceeds $Y$ in memory order
  \item \chpl{t = begin\{X\}} start a new task named $t$ to execute $X$
  \item \chpl{wait(t_1..t_n)} wait for tasks $t_1..t_n$ to complete
  \item $on L$ run the remainder of this task on another locale
\end{itemize}

0. Task creation and task waiting create a conceptual tree of program
statements. Thus, the body of the tasks, task creation, and task wait
operations create a partial order $<_p$ on memory operations. We will call
$<_p$ \textit{program order}. For the purposes of this section, the statements
in the body of each Chapel task will be implemented in terms of \textit{load},
\textit{store}, and \textit{atomic operation}.

\begin{itemize}
  \item If we have a program snippet without tasks, such as \chpl{X; Y;}, where $X$ and $Y$ are memory operations (ie one of $L(a)$,$S(a)$, or $A(a,o)$), then $X <_p Y$.
  \item The program \chpl{X; begin\{Y\}; Z;} implies $X$ $<_p$ $Y$ but there is no particular relationship between $Y$ and $Z$ in program order.
  \item The program \chpl{t = begin\{Y\}; wait(ti); Z;} implies $Y$ $<_p$ $Z$
  \item $X$ $<_p$ $Y$ and $Y$ $<_p$ $Z$ imply $X$ $<_p$ $Z$
\end{itemize}

For the purposes of describing this memory model, it is assumed that Chapel
programs will be translated into sequences of \textit{memory operations},
\chpl{begin}, and \chpl{wait} statements. The translation of a Chapel program
into a sequence of \textit{memory operations} must preserve sequential program
semantics. That is, if we have a snippet of a a Chapel program without task
operations, such as \chpl{X; Y;}, the statements $X$ and $Y$ will be converted
into a sequence of \textit{load}, \textit{store}, and \textit{atomic
operations} in a manner that preserves the behavior of a serial portion of the program. Operations
in one statement preceed operations in the next statement. Thus,
$X=x_1,x_2,...$ and $Y=y_1,y_2,...$ where $x_i$ and $y_j$ are each a sequence
of \textit{load}, \textit{store}, or \textit{atomic operations} and we have
$x_i <_p y_j$ in $<_p$.

Likewise, for the purposes of this memory model, Chapel's parallelism keywords
are viewed as a sequence of operations including the primitives of starting a
task (\chpl{begin}) and waiting for some number of tasks
(\chpl{wait(t_i..t_n)}). In particular:

\begin{itemize}
  \item \chpl{forall} creates some implementation-dependent number of tasks $m$ with \chpl{t_i = begin\{some-loop-bodies\}} to execute some number of loop iterations and waits for them to complete with \chpl{wait(t_1..t_m)}.
  \item \chpl{coforall} creates one task per loop iteration (\chpl{t_i = begin\{loop-body\}} for all loop iterations $i=1..n$) and then waits for them all to complete (with \chpl{wait(t_1..t_n)}).
  \item \chpl{cobegin} creates one task per contained statement (\chpl{t_i = begin\{X_i\}} for all contained statements $X_i$ with $i=1..n$) and then waits for them all to complete (with \chpl{wait(t_1..t_n)}).
  \item \chpl{begin} creates a task to execute the contained statement and adds it to a list of tasks executing in dynamic scope (\chpl{t = begin\{X\}}).
  \item \chpl{sync} statement waits for any of the $n$ tasks $t_i$ created in the dynamic scope of the sync statement with \chpl{wait(t_1..t_n)}.
\end{itemize}

For the purposes of the memory model, the \chpl{on} statement does not have any
special meaning for program order. In other words, replacing the \chpl{on}
statement with a block statement would result in a program with equivalent
program order. Put another way, the \chpl{on} statement has sequential semantics.

Also note that \chpl{sync} variables will have memory consistency behavior
equivalent to a sequence of operations on \chpl{atomic} variables and so are
not separately described here. In particular, for the purposes of memory
consistency, a operations on a \chpl{sync} variable will have the same
consistency implications as if an \chpl{atomic} variable were used with
SC as a lock to protect modifications of the \chpl{sync}
variable's value or full/empty state.


1. All tasks insert SC atomic operations into the order $<_m$ respecting
program order as described in the rules below. Note that atomic operations with
relaxed semantics (relaxed, acquire, or release semantics which are less strict
than SC) do not create a total order. We will discuss relaxed atomic operations
below.

\begin{itemize}
  \item If $A(a)<_pA(b)$ then $A(a)<_mA(b)$
\end{itemize}

2. Every SC atomic operation gets its value from the last SC atomic operation before it to the same address in the total order $<_m$:
\begin{itemize}
  \item Value of $A(a)$ = Value of $MAX_{<_m} \{A'(a)|A'(a) <_m A(a) \}$
\end{itemize}

3. For data race free programs, all tasks insert loads and stores into the
total order $<_m$ respecting the below rules which preserve the order of loads
and stores relative to atomic operations.  Note that if a program contains data
races, the \textit{memory order} is not an order at all for the racy loads
$L(a)$ and stores $S(a)$ because these loads and stores do not have to be
completed at once (in other words, two racing stores to the same address could
result in a value being written that is neither of the written values but is a
combination of the two writes). 
\begin{itemize}
  \item If $L(a)<_pA(b)$ then $L(a)<_mA(b)$
  \item If $S(a)<_pA(b)$ then $S(a)<_mA(b)$
  \item If $A(a)<_pL(b)$ then $A(a)<_mL(b)$
  \item If $A(a)<_pS(b)$ then $A(a)<_mS(b)$
\end{itemize}

4. For data race free programs, all tasks insert their loads and stores to the
same address into the order $<_m$ respecting the following rules which preserve
sequential program semantics:

\begin{itemize}
  \item If $L(a) <_p L'(a)$ then $L(a) <_m L'(a)$
  \item If $L(a) <_p S(a)$ then $L(a) <_m S(a)$
  \item If $S(a) <_p S'(a)$ then $S(a) <_m S'(a)$
\end{itemize}

5. For data race free programs, every load gets its value from the last store before it to the same address in the total order $<_m$:
\begin{itemize}
  \item Value of $L(a)$ = Value of $MAX_{<_m}$ \{ $S(a)|S(a)$ $<_m$ $L(a)$ or $S(a)$ $<_p$ $L(a)$ \}
\end{itemize}

TODO: Describe these with a table of what can be reordered with what.

Relaxed Atomics

This section describes the behavior of programs using relaxed atomic operations
- those with acquire, release, or relaxed consistency rather than SC. The
behavior of these relaxed atomics is not intended to differ from the behavior
described in the C standard (section 5.1.2.4, 7.17.3, and 7.17.4).

For all atomic operations, including those using relaxed consistency:
 - The operations on the atomics do complete in a total order \it{for each address}. In other words, at any given moment, an atomic at address $a$ updated with relaxed atomics by any number of threads will either have the result of the update or not. It cannot have an intermediate state.

For atomics with relaxed consistency:
 - The operations on the atomics do not imply any ordering of other atomics, loads, or store
 - The operations on the atomics do not necessarily complete in a total order.

Atomics with acquire consistency have a pairwise relationship...
TODO: A(a,o) with Acquire and Release. C standard section 5.1.2.4 p 20, 7.17.3 and 7.17.4 p 277. C++ standard section 1.10 p11 and and 29 p 1113

Justification Note.

Chapel's memory model does not include the \textit{write atomicity} or
\textit{store atomicity} property for general variable writes for two reasons.
First, an RDMA message could be partway through copying some data when another
thread reads that data. Second, two portions of the machine could have some
local hardware caches that complete writes for other cores at different times
from further away cores as allowed by the C11 or C++11 memory models. Sorin,
Hill and Wood define \textit{write atomicity} as the property that a store
operation is logically seen by all other cores at once (section 5.5.2). This
property is upheld for seq\_cst operations on \chpl{atomic} variables by
construction (since these are in a total order). However, \textit{write
atomicity} is not upheld for all atomic operations. For a given moment in any
task's execution, a write to any \chpl{atomic} variable write is either
entirely completed or entirely not yet started. This property is guaranteed for
any \chpl{atomic} variables with any ordering constraint including relaxed.
However, the property that a given write is seen by all other tasks at once is
not upheld for atomic operations unless the seq\_cst ordering is used.

For a distributed memory system, the straightforward implementation of SC atomics or sync variables will be sequentially consistent. The straightforward implementation is to follow these rules:
 - every task issues these SC atomic operations in program order
 - each SC atomic operation is performed only at the 'home' of the atomic variable in question (e.g. with an Active Message or with network hardware support)
 - each task does not start another operation until the SC atomic operation it is working on has completed on the 'home' of the atomic variable
In that case, the reasoning in Adve Hill "Implementing Sequential Consistency in Cache-Based Systems" applies and the executions of these synchronization operations will be sequentially consistent.

If the SC atomic operations themselves are sequentially consistent and the
program is free of data races, the load and store operations will also be
sequentially consistent. The reasoning here is that:
 - The load and store operations cannot be reordered across SC atomic operations (Rule 3 above).
 - Access by two different tasks to the same memory location, when at least one of the accesses is a store, is a race condition. In order to avoid race conditions, and preserve SC semantics accesses to the same memory location (when one of the accesses is a store) must be mediated by SC atomic operations.
 - The SC atomic operation constrains the order of the load and store operations so that they appear as a total order.

Sketch of proof. Suppose that a data race free program produces a non-SC
outcome. That would mean that there is no total ordering on the loads or stores
to a particular memory location. If the memory location is never updated in the
relevant program region, the loads could be considered to be in any order in
the total order and provide the same result. If at least one of the operations
is a store, and it is not already constrained by the SC atomic operations, then
there is a race condition.

%(Corrolary 1: LLVM optimizations and C program optimizations are OK because C programs cannot reorder SC atomic ops)
%(Corralary 2: Cache is OK because:
% - any communication not mediated by atomics constitutes a data race
% - use of SC atomics causes cache flush/invalidate)

%Adve phd thesis
%Scheurich and Dubios ScD87 and Sch89 - sufficient condition for sequential consistency.
%Collier proved .... writes in same order equivalent to atomically... thus system ... is sequential consistenc. Col84-92 and DuS90.
%LHH91 also do it. All writes are seen in the same order by all processors -> sequentially consistent.
%\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.8176&rep=rep1&type=pdf}: An execution is consistent with respect to a consistency model... and these 3 invariants are sufficient to guarantee memory consistency defined in this way: Uniprocessor Ordering, Allowable Reordering, and Cache Coherence.
%i1. typical implementation is coherent:
% coherent "a coherent system must appear to execute all thread's loads and stores to a single memory location in a total order that respects the program order of each thread".
% Sufficient Conditions for SC:
%  - Every processor issues memory ops in program order
%  - Processor must wait for store to complete before issuing next memory operation
%  - After load, issuing proc waits for load to complete, and store that produced value to complete before issuing next op
%Scheurich Dubois and Brgiss 1988 Synchronization, Coherence, and Event Ordering in Multiprocessors.
%\url{ftp://ftp.cs.wisc.edu/markhill/Papers/icpp90_seqcon.pdf} sufficient conditions for SC including the above.
%\url{http://www.cs.berkeley.edu/~kubitron/cs252/lectures/lec20-sharedmemory3.pdf} has a picture of "exclusion zone" and argument for why processor atomics -> sequential consistency.

%\begin{chapelexample}{syncFenceFlag}
\begin{example}
  In this example, a synchronization variable is used to (a) ensure that
  all writes to an array of unsynchronized variables are complete, (b)
  signal that fact to a second task, and (c) pass along the number of
  values that are valid for reading.

  The program
\begin{chapel}
var A: [1..100] real;
var done(*\texttt{\$}*): sync int;           // initially empty
cobegin {
  { // Reader task
    const numToRead = done(*\texttt{\$}*);   // block until writes are complete
    for i in 1..numToRead do
      writeln("A[", i, "] = ", A[i]);
  }
  {  // Writer task
    const numToWrite = 23;     // an arbitrary number
    for i in 1..numToWrite do
      A[i] = i/10.0;
    done(*\texttt{\$}*) = numToWrite;        // fence writes to A and signal done
  }
}
\end{chapel}
  produces the output
\begin{chapelprintoutput}{}
A[1] = 0.1
A[2] = 0.2
A[3] = 0.3
A[4] = 0.4
A[5] = 0.5
A[6] = 0.6
A[7] = 0.7
A[8] = 0.8
A[9] = 0.9
A[10] = 1.0
A[11] = 1.1
A[12] = 1.2
A[13] = 1.3
A[14] = 1.4
A[15] = 1.5
A[16] = 1.6
A[17] = 1.7
A[18] = 1.8
A[19] = 1.9
A[20] = 2.0
A[21] = 2.1
A[22] = 2.2
A[23] = 2.3
\end{chapelprintoutput}
%\end{chapelexample}
\end{example}


\begin{chapelexample}{syncSpinWait.chpl}
One consequence of Chapel's memory consistency model is that a task cannot spin-wait on a
variable waiting for another task to write to that variable.  The behavior of
the following code is undefined:

\begin{chapelpre}
if false { // }
\end{chapelpre}
\begin{chapel}
var x: int;
cobegin with (ref x) {
  while x != 1 do ;  // spin wait
  x = 1;
}
\end{chapel}
\begin{chapelnoprint}
// {
}
\end{chapelnoprint}
In contrast, spinning on a synchronization variable has well-defined
behavior:
\begin{chapel}
var x(*\texttt{\$}*): sync int;
cobegin {
  while x(*\texttt{\$}*).readXX() != 1 do ;  // spin wait
  x(*\texttt{\$}*).writeXF(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

In this code, the first statement in the cobegin statement executes a
loop until the variable is set to one.  The second statement in the
cobegin statement sets the variable to one.  Neither of these
statements block.
\end{chapelexample}

\begin{chapelexample}{atomicSpinWait.chpl}
Atomic variables provide an alternative means to spin-wait. Atomic variables include a waitFor method that will block the calling thread until a read of the atomic value matches a particular value. For example:

\begin{chapel}
var x: atomic int;
cobegin with (ref x) {
  x.waitFor(1);
  x.write(1);
}
\end{chapel}
\begin{chapeloutput}
\end{chapeloutput}

\end{chapelexample}


\begin{future}
Upon completion, Chapel's atomic statement~(\rsec{Atomic_Statement}) will serve as
an additional means of correctly synchronizing between tasks.
\end{future}

