#!/bin/csh -f
#
#
# Main test script for Chapel compiler -- based on testing system from
# the ZPL testing system, originally developed by Jason Secosky with
# later mods by Brad Chamberlain, Sung-Eun Choi, Steve Deitz, and
# E Christopher Lewis.
#
# Executive Summary: The overall flow of the testing system is that it
# will recursively descend into subdirectories looking for Chapel
# programs to compile and run (*.chpl) as well as for other tests to
# run (sub_test).  The output of these runs will typically be logged
# in a file stored in the Logs/ subdirectory, as will a summary of the
# errors reported (determined by grepping for the string "[Error", so
# don't have your program print this out) and the status of "future"
# tests -- those that have never worked, but are checked in for the
# purpose of sharing and placeholding.
#
# Here's the directory structure:
#
#   Bin/     -- contains binary files/scripts required by the testing
#               system, such as the timedexec script which kills a
#               test if it takes too long
#   Logs/    -- logs of testing runs go here by default
#   Samples/ -- sample tests go here; these are for illustration only
#               and won't be run by default.  To try running the
#               test system against these samples, use:
#                      start_test -startdir ./Samples
#   Share/   -- a place to put codes to share with other developers.
#               These will not be run by default.
#   */       -- all other directories will contain tests
#
# The start_test script kicks off all the action.  With no arguments,
# it will run all the tests using the defaults.  The '-h' option lists
# the options that the script accepts and the default values.  Current
# options are:
#
#   option     argument               default value
#   ---------  ---------------------  -------------
#   -compiler  <compiler executable>  ../bin/<platform>/chpl
#   -compopts  <option list>          ""
#   -execopts  <option list>          ""
#   -startdir  <test subdir>          .
#   -norecurse
#   -onetest   <single test>          N/A
#   -logfile   <log filename>         ./Logs/<username>.<platform>.log
#   -valgrind
#   -notee
#   -futures
#   -futuresonly
#   -performance
#   -comm [none|gasnet]
#   -interpret (DEPRECATED)
#
# These options can be used with a double-dash as well 
# (e.g. --compopts).
#
# The -compiler option allows the user to specify the compiler to test
# if it is something other than the obvious one in the current CVS
# structure.  This lets one run other people's compilers, old copies
# of compilers, etc.
#
# The -compopts option allows the user to specify a set of compiler
# options that should be used on every invocation to the compiler.
# Additional compiler options can be specified on a directory-by-
# directory basis.
#
# The -execopts option allows the user to specify a set of execution
# options that should be used on every invocation of a program.  As
# with compiler options, these can be ammended in each subdirectory.
#
# The -startdir option allows the user to specify a subdirectory of
# the testing system to start in (and limit itself to).  Assuming
# that tests are sorted into subdirectories by useful features, this
# allows you to run a subset of the tests easily.
#
# The -norecurse flag requests that the starting directory be tested
# but that its subdirectories not be visited recursively.  This is
# useful for pinpointing the testing of a single directory with
# -startdir when you do not want to also visit its subdirectories,
# if any.
#
# The -onetest flag requests that only a single test be tested.  The
# test should be a full path to the test, and should have a .chpl file
# extension.
#
# The -logfile option indicates where the log of the test run should
# be kept.  By default it's based on the user's name.  If the log
# file already exists, you will be prompted at the beginning of the
# run whether you want to delete that file or not.  At the end of
# the run, a second log file named <logfile>.summary will be
# generated containing only the Errors that were logged.
#
# The -valgrind option specifies that the compiler and generated
# executable should be run using valgrind in order to find errors.
# The -valgrindexe option specifies that the generated executable
# should be run using valgrind, but not the compiler.
#
# The -notee option indicates that "tee" should not be used.  This was
# added due to a sense that tee was breaking the nightly regression
# runs on the Cygwin platform.
#
# The -futures option specifies that the testing system should test
# both future and non-future tests.  By default, future tests are
# skipped over.  The -futuresonly option specifies that the testing
# system should test only the future tests and not the others.
#
# The -performance flag specifies that the testing system should
# search for and execute performance tests within the testing
# system.  A test foo.chpl is considered a performance test if a
# file "foo.perfkeys" exists.  This file contains a string per
# line that is used to read performance-related cues from the
# test's compiler+execution output file.  For example, if a test
# generates a line "Time: x.yz seconds", putting the key "Time:"
# into its .perfkeys file would cause the testing system to grab
# the value "x.yz" out of the output file (all non-whitespace 
# characters after the key string until the next whitespace 
# character).  These data values will then be written to a file
# named $CHPL_TEST_PERF_DIR/<machine name>/foo.dat (where
# $CHPL_TEST_PERF_DIR is "." by default) in a  TAB-separated manner 
# for graphing or display using gnuplot or Excel.
#
# When running with the -performance flag, compiler and executable
# options are specified in files with .perfcompopts and 
# .perfexecopts since they will most likely require different
# compiler options (to specify optimization, turn off bounds
# checking) and execution options (to request timings and other
# unpredictable values to be printed out, whereas they're likely
# to be squelched in the correctness run).
#
# Success/failure summaries after a performance run indicate
# whether all the performance keys for a test were matched or not.
# In particular, a failure indicates that a test's performance
# keys couldn't all be found.
#
# The -interpret option requests that the compiler be executed in
# interpreter mode.  This will cause the test system to invoke the
# compiler to interpret the program rather than compile and run it.
# Future tests, those utilizing executable options (execopts), and
# those that are not executed (noexec) tests are skipped when this
# flag is invoked.  In addition, a directory can be requested to
# be skipped by dropping a NOINTERP file into it.  Because the
# Chapel compiler's interpreter feature is currently disabled, this
# flags is currenty deprecated.
#
# The -comm option requests that testing be performed with respect to
# a specific comm layer.  At the time of this writing, none and gasnet
# are the two supported options.  Choosing something other than "none"
# will cause "-nl 1" to be prefixed to the generated executable's
# command-line options to set the number of locales.  Setting the
# number of locales to something other than 1 can be done using
# NUMLOCALES and <testname>.numlocales files in the testing
# directories themselves.  Using these mechanisms to set the number of
# locales to 0 will cause the testing system to not use the -nl flag
# by default.  Any directory or test that has a NUMLOCALES file, or
# that has a .numlocales file specifying a number other than 0 or 1
# will be skipped when using "none" as the communic layer (either
# explicitly or by not specifying a -comm flag).
#
# By default, setting up a subdirectory for testing simply consists
# of creating the directory, putting Chapel (.chpl) source files
# into it and an expected output file (.good) for each source file
# (using the same base name).  Upon reaching such a directory, the
# testing system will run the specified compiler on each Chapel
# source file using the specified compiler options, then (assuming
# the compile completed successfully, execute the resulting program 
# using the specified execution options.  The output from both the
# compilation and the execution are concatenated and diff'd against
# the .good file.  This allows programs that are supposed to generate
# errors, warnings, and correct programs to all be tested using the
# same mechanisms.  
#
#If the output for a test varies by machine, communication layer, or
# platform name, files named <testname>.<machname>.good,
# <testname>.comm-<commlayer>.good, or <testname>.<platform>.good can
# be used to specify the output for such tests, where "machname" is
# the output of "uname -n" with any "." qualifiers after the machine
# name stripped off, commlayer is the communication layer being tested
# (e.g., "none" or "gasnet") and "platform" is the value of
# $CHPL_PLATFORM.  The most specific good file will be used, in the
# order: machname, commlayer, platform, generic.
#
# Tests that are not yet expected to work can be marked as such
# by creating a <testname>.future file.  The presence of the
# .future file will prevent the test from counting toward our
# nightly successes and regressions, and is intended to allow
# tests to be checked in to share them between multiple developers
# in-line with other tests that work.  In general, once a test
# is working and stable, its future file should be removed and 
# should not be re-added (for future failures of the test should
# be counted as regressions).  The .future file should contain
# the userid of the developer whose court it's in on the first
# line (this will appear in the test system's summaries), or some
# other categorization of what the future is dependent on to pass.
# Subsequent lines can contain notes and will be ignored by the
# testing system.
#
# As mentioned above, running the test system in interpreter mode
# does not attempt to execute tests with .future files.  The
# interpreter mode of the testing system has a parallel mechanism
# for creating future tests using a .ifuture file (the "i"
# stands for interpreter)
#
# If the test-writer wants to redirect standard input from a file,
# they may do so by supplying a .stdin file with the same base
# name as the test itself (e.g., if mytest.stdin exists, it will
# be piped into stdin when running the executable created from
# mytest.chpl).  If no such file exists, standard input is piped
# from /dev/null (i.e., tests can't read from the console...)
#
# Particular subdirectories can also be customized if necessary.
# Note that such customizations are not inherited recursively by
# further subdirectories, but apply only to the directory in
# question (we might consider changing this in future versions).
# The customizations are as follows:
#
#   - if the subdirectory contains an executable sub_test script,
#     that script will be used to run the tests in that directory
#     rather than the default sub_test script (located in the Bin
#     directory).  A sub_test script may take whatever actions it
#     wants, and is simply expected to generate any errors using
#     the "[Error ...]" format so that it will show up in the
#     summary.  Similarly, the script should generate any warnings
#     or successful tests using "[Warning ...]" "[Success ...]"
#     messages for consistency.  The sub_test script will be
#     sent two arguments: (1) the compiler to use, and (2) the
#     location of this main test/ directory.  The compiler and
#     execution options will be stored in environment variables
#     named COMPOPTS and EXECOPTS, respectively.
#
#  - if the subdirectory contains a NOTEST file, that directory
#    will not be considered for testing.  This can be useful for
#    disabling subdirectories containing tests that don't work
#    yet, or subdirectories that contain input files for other
#    tests (though they will also be ignored if they fail to
#    contain any .chpl files...).  This may also be selected for
#    a single test, foo.chpl, by creating a foo.notest file.
#
#  - if the subdirectory contains a NOEXEC file, any executables
#    built in that directory will not attempt to be executed.
#    Rather, only the compiler output will be diffed against the
#    expected output (note that when a compile fails, this will
#    also happen automatically).  This may also be selected for
#    a single test, foo.chpl, by creating a foo.noexec file.
#
#  - if the subdirectory contains a NOVGRBIN file and the -valgrind
#    flag was used, the generated binary will not be run using 
#    valgrind (the compiler still would be).
#
#  - if the subdirectory contains a COMPOPTS or EXECOPTS file,
#    the options listed in that file will be added to the compiler
#    and execution options for that subdirectory.  In addition, 
#    a test named foo.chpl can add its own compilation and execution
#    options by specifying them in foo.compopts and foo.execopts.
#
#  - also added support for a LASTCOMPOPTS file that contains
#    compiler options to be added after the source file.  Thus:
#    ./chpl <-compopts> <COMPOPTS> source.chpl <LASTCOMPOPTS>
# 
#  - if the subdirectory contains a COMPSTDIN file, the contents of
#    COMPSTDIN will be piped into the execution of the compile step as
#    stdin.  I can add a similar feature for the execution step as
#    soon as there's need for it.
#
#  - if the subdirectory contains a CATFILES file, then the files
#    listed in that file will be concatenated to the end of the
#    compiler/execution output for each test.  For tests that
#    generate files (either as a result of the compilation or
#    as part of the executable's behavior), this can be used to
#    ensure that the generated file's contents are correct without
#    writing a specialized sub_test script.  Again, this file should
#    be a single line with no linefeeds.  In addition, a test named 
#    foo.chpl can add its own concatenation files by specifying them 
#    in foo.catfiles.
#
#  - if the subdirectory contains an executable PREDIFF file, that
#    file will be executed prior to running any diff command and
#    will be sent three arguments: 1) the name of the current test,
#    2) the name of the output file that the diff is going to
#    be taken against, and 3) the compiler being used.  A 
#    test-specific PREDIFF script can be added  by using a foo.prediff
#    script.
#
#  - similarly, actions desired before running the generated
#    executable can be specified using a PREEXEC or foo.preexec
#    script.  (other such commands can be added to various
#    stages of the sub_test script on request).
#
#  - if the subdirectory contains a TIMEOUT file, then that file
#    will be read to determine the number of seconds that the tests
#    in the directory should be allowed to run before being killed.
#    The default is currently 5 minutes.  A test foo.chpl can also
#    override the timeout just for itself by supplying a timeout
#    value in a foo.timeout file.
#
#  - subdirectory-specific .cvsignore files can also be very 
#    helpful so that files generated during testing won't clutter
#    the results of a cvs -nq update command.
#
# An alternate way of setting up a subdirectory for testing is to
# use a Makefile-driven test.  This is most useful for tests that
# require several source files (to avoid littering the directory
# with .notest files), or a very complicated build procedure.  To
# set up such a test, the following steps should be taken:
#
# 1) add a file USEMAKE to the directory
#
# 2) add a Makefile to the directory that contains a "test" target:
#    - the Makefile should refer to the chpl compiler using $(CHPL),
#      which will be fed to the Makefile by the testing script.
#    - make the Makefile execute any binaries using -nl 1 (or
#      something even more intelligent to make it work well with
#      parallel testing.
#    - you will probably want to squelch the Makefile's echoing of
#      commands using the @ prefix
#    - and to avoid breaking due to error status codes using the -
#      prefix?
#
# 3) add a file make.good to the directory indicating what the output
#    of "make test" ought to be
#
# Note that for such directories, performance testing is still done
# using the usual mechanism: by providing .perfkeys, .perfcompopts,
# and .perfexecopts files.  This could be replaced by a "perftest"
# target if there is a compelling reason to do so.
#
# The Makefile mode of testing is fairly new; please send bugs and
# feature requests to bradc@cray.com
#
# Also worth describing here is the start_clean script which walks
# the directory structure in a similar manner and cleans up --
# removing the generated executables, core files, and *.tmp files
# which store any mismatching output.  The user can also specify
# subdirectory-specific things to clean up using a CLEANFILES
# file that lists other targets to remove (called with the -rf
# flag, so subdirectories will work here as well).  The idea is
# that after start_clean runs, the testing system should be left
# in a state pretty close to what's checked into the CVS tree.
#
# Again, to see a sample run of the testing system, look through
# the Samples/ directory, then run:
#
#     ./start_test -startdir Samples
#
# and inspect the Samples/ and Logs/ subdirectories to see what
# was generated.  Then use:
#
#     ./start_clean -startdir Samples
#
# to clean back up again.
#

set user = `whoami`

#
# unset things that users may have set in their environment
#
unsetenv CHPL_HOME
unsetenv CHPL_DEVELOPER
unsetenv CHPL_COMM

# Commented this out, because it only seems useful in shared environments:
## Make sure that other testers can modify what another 
## tester does
#umask 002

#
if (! -e ./Logs) then
   mkdir ./Logs
endif
set logtmp = ./Logs/$user.tmp.log
set datestr = `date +"%y%m%d.%H%M%S"`

# remember what directory to return to (the current working directory)
set testdir = $cwd

if ( ${?CHPL_CROSS_PLATFORM} != "0" ) then
  set platform = "$CHPL_CROSS_PLATFORM";
else
  set platform = `../util/platform`;
endif

#
# some sets to get locale, environment reasonable
#
if ($platform != "sunos") then
    setenv LC_ALL C
    setenv LANG en_US
endif
limit stacksize 8192k


set execopts = ""
set compiler = ""
set compopts = ""
set startdir = "$testdir"
set onetest = ""
set onetestpath = ""
set logfile = "$testdir/Logs/$user.$platform.log"
set valgrind = 0
set valgrindexe = 0
set interpret = 0
set performance = 0
set testfutures = 0
set testfuturesonly = 0
set testnotests = 0
set recurse = 1
set tee = "tee"
set numlocales = 0
set comm = "none"

while ( $#argv > 0 )
	switch ( $argv[1] )
	case -execopts:
	case --execopts:
		shift
		set execopts = "$execopts $argv[1]"
		shift
		breaksw
	case -compiler:
	case --compiler:
		shift
		set compiler = $argv[1]
		shift
		breaksw
	case -compopts:
	case --compopts:
		shift
		set compopts = "$argv[1]"
		shift
		breaksw
	case -startdir:
	case --startdir:
		shift
		set startdir = $argv[1]
		shift
		breaksw
	case -logfile:
	case --logfile:
		shift
		set logfile = $argv[1]
		shift
		breaksw
	case -valgrind:
	case --valgrind:
		shift
		set valgrind = 1
		breaksw
        case -futures:
        case --futures:
                shift
                set testfutures = 1
                breaksw
        case -futuresonly
        case --futuresonly
                shift
                set testfuturesonly = 1
                breaksw
        case -i:
        case --interpret:
        case -interpret:
                shift
                set interpret = 1
                breaksw
        case --performance:
        case -performance:
                shift
                set performance = 1
                breaksw
        case -norecurse:
        case --norecurse:
                shift
                set recurse = 0
                breaksw
        case -onetest:
        case --onetest:
                shift
                set onetestpath = $argv[1]
                set testfutures = 1
                set testnotests = 1
                shift
                breaksw
        case -valgrindexe:
        case --valgrindexe:
                shift
                set valgrindexe = 1
                breaksw
        case -notee:
        case --notee:
                shift
                set tee = "$testdir/Bin/cat2File"
                breaksw
        case -comm:
        case --comm:
                shift
                set comm = $argv[1]
                shift
                if ($comm != "none") then
                    set numlocales = "1"
                endif
                breaksw
	case -h:
	case -help:
        case --help
		echo Usage and defaults\:
		echo "     start_test"
		echo "          -compiler ../bin/$platform/chpl"
		echo "          -compopts" \"\"
		echo "          -execopts" \"\"
                echo "          -onetest"
		echo "          -startdir ."
                echo "          -norecurse"
                echo "          -performance"
                echo "          -futures"
                echo "          -futuresonly"
		echo "          -valgrind"
                echo "          -valgrindexe"
                echo "          -notee"
                echo "          -comm [none|gasnet]"
#                echo "          -interpret (or -i)"
		echo "          -logfile ./Logs/$user.$platform.log"
		echo "          -h, -help"
		exit 0
		breaksw
	default:
		echo \[ERROR: Unknown command line parameter \"$argv[1]\", aborting.\]
		exit 1
		breaksw
	endsw
end

echo \[Starting Chapel regression tests - $datestr\] |& $tee $logtmp
echo \[pwd: \"$testdir\"\] |& $tee -a $logtmp

echo \[platform: $platform\] |& $tee -a $logtmp
# see if valgrind is on.  If it is, reset the compiler
if ($valgrind) then
    echo \[valgrind: ON\] |& $tee -a $logtmp
    setenv CHPL_TEST_VGRND_COMP on
    setenv CHPL_TEST_VGRND_EXE on
    which valgrind > /dev/null
    if ( $status != 0 ) then
	echo ERROR: can\'t find valgrind |& $tee -a $logtmp
	exit 1
    endif
else
    setenv CHPL_TEST_VGRND_COMP off
    if ($valgrindexe) then
        echo \[valgrind: EXE only\] |& $tee -a $logtmp
        setenv CHPL_TEST_VGRND_EXE on
    else
        echo \[valgrind: OFF\] |& $tee -a $logtmp
        setenv CHPL_TEST_VGRND_EXE off
    endif
endif

if ($compiler == "") then
  set compiler = "../bin/$platform/chpl"
endif

if ($interpret) then
    echo \[interpreter: ON\] |& $tee -a $logtmp
    set compopts = "-i $compopts"
    setenv CHPL_TEST_INTERP on
else
    echo \[interpreter: OFF\] |& $tee -a $logtmp
    setenv CHPL_TEST_INTERP off
endif

if ($performance) then
    echo \[performance tests: ON\] |& $tee -a $logtmp
    setenv CHPL_TEST_PERF on
else
    echo \[performance tests: OFF\] |& $tee -a $logtmp
endif


# if compiler exists then get absolute path name for
if ( -f $compiler && -x $compiler ) then
	pushd `dirname $compiler` >& /dev/null
	set compiler = $cwd/`basename $compiler`
	popd >& /dev/null

	echo \[compiler: \"$compiler\"\] |& $tee -a $logtmp
else
	echo \[Cannot find or execute compiler: \"$compiler\"\] \
		|& $tee -a $logtmp
        exit 1
endif

echo \[compopts: \"$compopts\"\] |& $tee -a $logtmp

echo \[execopts: \"$execopts\"\] |& $tee -a $logtmp

echo \[comm: \"$comm\"\] |& tee -a $logtmp
setenv CHPL_COMM $comm

if ($numlocales == "0") then
    echo \[numlocales: \"\(default\)\"\] |& tee -a $logtmp
else
    echo \[numlocales: \"$numlocales\"\] |& tee -a $logtmp
endif

if ($onetestpath != "") then
    if ($startdir != $testdir) then
        echo \[Using -startdir with -onetest doesn\'t make sense\] |& $tee -a $logtmp
        exit 1
    endif
    if ( -r "$onetestpath") then
        set startdir = `dirname $onetestpath`
        set onetest = `basename $onetestpath`
        set recurse = 0
    else
        echo \[Can\'t access test: \"$onetestpath\"\] |& $tee -a $logtmp
        exit 1
    endif
endif

# if startdir exists then get absolute path name for
if ( -d "$startdir" && -x "$startdir" ) then
	pushd "$startdir" >& /dev/null
	set startdir = "$cwd"
	popd >& /dev/null

        if ($recurse == 1) then
            echo \[startdir: \"$startdir\"\] |& $tee -a $logtmp
        else
            echo \[startdir \(nonrecursive\): \"$startdir\"\] |& $tee -a $logtmp
            if ($onetest != "") then
                echo \[onetest: $onetest\] |& $tee -a $logtmp
            else
            endif
        endif
else
	echo \[Permission denied for starting directory: \"$startdir\"\] \
		|& $tee -a $logtmp
	exit 1
endif

#if logfile directory exists, then get absolute path for
if ( -d `dirname $logfile` && -x `dirname $logfile` ) then
	pushd `dirname $logfile` >& /dev/null
	set logfile = `pwd`/`basename $logfile`
	popd >& /dev/null

	echo \[logfile: \"$logfile\"\] |& $tee -a $logtmp
else
	echo \[Permission denied for logfile directory: \"`dirname $logfile`\"\] \
		|& $tee -a $logtmp
	exit 1
endif

if ( -w $logfile ) then
	echo ""
	echo \[Removing log file with duplicate name \"$logfile\"\]
	rm -f $logfile
endif

# Move temp log file ($logtmp) we have been accumulating to actual log file
#   now that we know the name of the actual log file
mv $logtmp $logfile


# if specified to start in a specific directory, start there
set basedir = $testdir
if ( $startdir != $cwd ) then
	set basedir = $startdir
	cd $basedir
endif

# execute script that is located in each directory
#   Recursively list all directories (have a : in the line)
#   Don't include output or RCS directories (-v says don't include)
#   Take off : with sed
#
if ($recurse == 1) then
    set dirs = `ls -R |& grep ":" | grep -v "\.:" | grep -v "Permission denied" | grep -v "CVS" | grep -v Bin | grep -v Logs | grep -v Samples | grep -v Share | grep -v perfdat | sed 's/://g'`
    # gotta add ./ on in case -startdir is specified
    set dirs = (./ $dirs)
else
    set dirs = (./)
endif

#echo "dirs is $dirs"
#echo "cwd is $cwd"
#exit 0

if ($testfuturesonly == 1) then
    setenv CHPL_TEST_FUTURES 2
else
    if ($testfutures == 1) then
        setenv CHPL_TEST_FUTURES 1
    else
        setenv CHPL_TEST_FUTURES 0
    endif
endif

if ($testnotests == 1) then
    setenv CHPL_TEST_NOTESTS 1
else
    setenv CHPL_TEST_NOTESTS 0
endif

foreach dir ($dirs)
	cd $basedir
	if ( -x $dir ) then
		pushd $dir >& /dev/null
		set dir = $cwd
		popd >& /dev/null

		cd $dir
	else
		echo \["Warning: Cannot cd into" $dir "skipping directory."\]|&\
			$tee -a $logfile
		continue
	endif
       	echo " " |& $tee -a $logfile
	set numtests = `(ls *.{chpl,v} |& grep -v "No match" | wc -l)`

        if (-e ./NUMLOCALES && $CHPL_COMM == "none") then
            echo \[Skipping NUMLOCALES directory: $dir \] |& $tee -a $logfile
            continue
        endif

	if ((! -e ./NOTEST && ($numtests != 0 || -x ./sub_test) && \
            (($CHPL_TEST_INTERP == "off") || (! -e ./NOINTERP && ! -e ./NOEXEC && ! -e ./EXECOPTS))) \
            || $onetest != "") then
		if (-x ./sub_test) then
		    set sub_test = ./sub_test
		else
		    set sub_test = $testdir/Bin/sub_test
		endif
                if (-x ./Bin/sub_clean) then
                    set sub_clean = ./Bin/sub_clean
                else
                    set sub_clean = $testdir/Bin/sub_clean
                endif
		setenv COMPOPTS "$compopts"
		setenv EXECOPTS "$execopts"
                setenv NUMLOCALES "$numlocales"
                if ($onetest != "") then
                    setenv CHPL_ONETEST "$onetest"
                endif
		echo "[Starting $sub_test `date`]" |& $tee -a $logfile
		$sub_clean "$testdir" |& $tee -a $logfile
		$sub_test "$compiler" "$testdir" |& $tee -a $logfile
	else
		echo \["No tests in directory " $dir\] |& \
			$tee -a $logfile
	endif
end

# return to directory where we started and remove lock
cd $testdir

echo \[Done with tests - `date +"%y%m%d.%H%M%S"`\] |& $tee -a $logfile
echo " " |& $tee -a $logfile

# Output grep to a temp file, don't want to infinite loop
set futuremarker = "^Future"
set errormarker = "^\[Error"
set successmarker = "^\[Success matching"

echo \[Test Summary - $datestr\] |& $tee $logfile.summary
grep "$errormarker" $logfile |& $tee -a $logfile.summary
grep "$futuremarker" $logfile |& $tee -a $logfile.summary

set successes = `grep "$successmarker" $logfile | wc -l`
set failures = `grep "$errormarker" $logfile | wc -l`
if ($testfutures == 0 && $testfuturesonly == 0) then
    echo \[\Summary: \#Successes = $successes \| \#Failures = $failures\] |& $tee -a $logfile.summary
else
    set futures = `grep "$futuremarker" $logfile | wc -l`
    echo \[\Summary: \#Successes = $successes \| \#Failures = $failures \| \#Futures = $futures\] |& $tee -a $logfile.summary
endif

echo \[END\] |& $tee -a $logfile.summary
cat $logfile.summary >> $logfile

echo ''
echo
